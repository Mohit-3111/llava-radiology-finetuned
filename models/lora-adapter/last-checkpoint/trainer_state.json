{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.2124702700181877,
  "eval_steps": 500,
  "global_step": 6500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0018654106235135009,
      "grad_norm": 0.25160977244377136,
      "learning_rate": 0.00019983212087297148,
      "loss": 2.8255,
      "step": 10
    },
    {
      "epoch": 0.0037308212470270018,
      "grad_norm": 0.1914217174053192,
      "learning_rate": 0.00019964558850960642,
      "loss": 2.714,
      "step": 20
    },
    {
      "epoch": 0.005596231870540503,
      "grad_norm": 0.15418438613414764,
      "learning_rate": 0.00019945905614624139,
      "loss": 2.7249,
      "step": 30
    },
    {
      "epoch": 0.0074616424940540035,
      "grad_norm": 0.1840512752532959,
      "learning_rate": 0.00019927252378287632,
      "loss": 2.6652,
      "step": 40
    },
    {
      "epoch": 0.009327053117567504,
      "grad_norm": 0.16974447667598724,
      "learning_rate": 0.0001990859914195113,
      "loss": 2.7628,
      "step": 50
    },
    {
      "epoch": 0.011192463741081005,
      "grad_norm": 0.14882975816726685,
      "learning_rate": 0.00019889945905614625,
      "loss": 2.7731,
      "step": 60
    },
    {
      "epoch": 0.013057874364594506,
      "grad_norm": 0.13962094485759735,
      "learning_rate": 0.00019871292669278122,
      "loss": 2.8088,
      "step": 70
    },
    {
      "epoch": 0.014923284988108007,
      "grad_norm": 0.19726411998271942,
      "learning_rate": 0.00019852639432941616,
      "loss": 2.78,
      "step": 80
    },
    {
      "epoch": 0.01678869561162151,
      "grad_norm": 0.16705185174942017,
      "learning_rate": 0.00019833986196605112,
      "loss": 2.7335,
      "step": 90
    },
    {
      "epoch": 0.01865410623513501,
      "grad_norm": 0.1677318513393402,
      "learning_rate": 0.00019815332960268606,
      "loss": 2.7359,
      "step": 100
    },
    {
      "epoch": 0.02051951685864851,
      "grad_norm": 0.15494975447654724,
      "learning_rate": 0.00019796679723932102,
      "loss": 2.7805,
      "step": 110
    },
    {
      "epoch": 0.02238492748216201,
      "grad_norm": 0.16430984437465668,
      "learning_rate": 0.000197780264875956,
      "loss": 2.7581,
      "step": 120
    },
    {
      "epoch": 0.024250338105675513,
      "grad_norm": 0.1727939248085022,
      "learning_rate": 0.00019759373251259095,
      "loss": 2.697,
      "step": 130
    },
    {
      "epoch": 0.026115748729189012,
      "grad_norm": 0.19214960932731628,
      "learning_rate": 0.0001974072001492259,
      "loss": 2.7741,
      "step": 140
    },
    {
      "epoch": 0.027981159352702515,
      "grad_norm": 0.17504073679447174,
      "learning_rate": 0.00019722066778586085,
      "loss": 2.731,
      "step": 150
    },
    {
      "epoch": 0.029846569976216014,
      "grad_norm": 0.16593265533447266,
      "learning_rate": 0.0001970341354224958,
      "loss": 2.7927,
      "step": 160
    },
    {
      "epoch": 0.03171198059972952,
      "grad_norm": 0.19656673073768616,
      "learning_rate": 0.00019684760305913076,
      "loss": 2.737,
      "step": 170
    },
    {
      "epoch": 0.03357739122324302,
      "grad_norm": 0.17638154327869415,
      "learning_rate": 0.00019666107069576572,
      "loss": 2.7367,
      "step": 180
    },
    {
      "epoch": 0.035442801846756515,
      "grad_norm": 0.1966821551322937,
      "learning_rate": 0.0001964745383324007,
      "loss": 2.7075,
      "step": 190
    },
    {
      "epoch": 0.03730821247027002,
      "grad_norm": 0.16826583445072174,
      "learning_rate": 0.00019628800596903562,
      "loss": 2.7762,
      "step": 200
    },
    {
      "epoch": 0.03917362309378352,
      "grad_norm": 0.1677425503730774,
      "learning_rate": 0.0001961014736056706,
      "loss": 2.7528,
      "step": 210
    },
    {
      "epoch": 0.04103903371729702,
      "grad_norm": 0.15943188965320587,
      "learning_rate": 0.00019591494124230553,
      "loss": 2.724,
      "step": 220
    },
    {
      "epoch": 0.04290444434081052,
      "grad_norm": 0.14277999103069305,
      "learning_rate": 0.00019572840887894052,
      "loss": 2.6924,
      "step": 230
    },
    {
      "epoch": 0.04476985496432402,
      "grad_norm": 0.16723571717739105,
      "learning_rate": 0.00019554187651557546,
      "loss": 2.7505,
      "step": 240
    },
    {
      "epoch": 0.046635265587837524,
      "grad_norm": 0.17318132519721985,
      "learning_rate": 0.00019535534415221042,
      "loss": 2.7134,
      "step": 250
    },
    {
      "epoch": 0.048500676211351026,
      "grad_norm": 0.16948634386062622,
      "learning_rate": 0.00019516881178884539,
      "loss": 2.7422,
      "step": 260
    },
    {
      "epoch": 0.05036608683486452,
      "grad_norm": 0.15554527938365936,
      "learning_rate": 0.00019498227942548032,
      "loss": 2.7672,
      "step": 270
    },
    {
      "epoch": 0.052231497458378025,
      "grad_norm": 0.1670721024274826,
      "learning_rate": 0.0001947957470621153,
      "loss": 2.7755,
      "step": 280
    },
    {
      "epoch": 0.05409690808189153,
      "grad_norm": 0.14928117394447327,
      "learning_rate": 0.00019460921469875025,
      "loss": 2.7602,
      "step": 290
    },
    {
      "epoch": 0.05596231870540503,
      "grad_norm": 0.15383726358413696,
      "learning_rate": 0.00019442268233538522,
      "loss": 2.7514,
      "step": 300
    },
    {
      "epoch": 0.057827729328918526,
      "grad_norm": 0.16890722513198853,
      "learning_rate": 0.00019423614997202016,
      "loss": 2.7226,
      "step": 310
    },
    {
      "epoch": 0.05969313995243203,
      "grad_norm": 0.16122044622898102,
      "learning_rate": 0.00019404961760865512,
      "loss": 2.7774,
      "step": 320
    },
    {
      "epoch": 0.06155855057594553,
      "grad_norm": 0.16040073335170746,
      "learning_rate": 0.00019386308524529006,
      "loss": 2.7438,
      "step": 330
    },
    {
      "epoch": 0.06342396119945903,
      "grad_norm": 0.1644749790430069,
      "learning_rate": 0.00019367655288192502,
      "loss": 2.792,
      "step": 340
    },
    {
      "epoch": 0.06528937182297254,
      "grad_norm": 0.16195949912071228,
      "learning_rate": 0.00019349002051856,
      "loss": 2.7489,
      "step": 350
    },
    {
      "epoch": 0.06715478244648604,
      "grad_norm": 0.1938580721616745,
      "learning_rate": 0.00019330348815519495,
      "loss": 2.7842,
      "step": 360
    },
    {
      "epoch": 0.06902019306999953,
      "grad_norm": 0.16591490805149078,
      "learning_rate": 0.0001931169557918299,
      "loss": 2.7695,
      "step": 370
    },
    {
      "epoch": 0.07088560369351303,
      "grad_norm": 0.2039692997932434,
      "learning_rate": 0.00019293042342846486,
      "loss": 2.8121,
      "step": 380
    },
    {
      "epoch": 0.07275101431702653,
      "grad_norm": 0.15372814238071442,
      "learning_rate": 0.0001927438910650998,
      "loss": 2.7571,
      "step": 390
    },
    {
      "epoch": 0.07461642494054004,
      "grad_norm": 0.1643943339586258,
      "learning_rate": 0.00019255735870173476,
      "loss": 2.7577,
      "step": 400
    },
    {
      "epoch": 0.07648183556405354,
      "grad_norm": 0.16475071012973785,
      "learning_rate": 0.00019237082633836972,
      "loss": 2.7091,
      "step": 410
    },
    {
      "epoch": 0.07834724618756704,
      "grad_norm": 0.14587673544883728,
      "learning_rate": 0.0001921842939750047,
      "loss": 2.7239,
      "step": 420
    },
    {
      "epoch": 0.08021265681108054,
      "grad_norm": 0.16499465703964233,
      "learning_rate": 0.00019199776161163962,
      "loss": 2.6694,
      "step": 430
    },
    {
      "epoch": 0.08207806743459405,
      "grad_norm": 0.15470954775810242,
      "learning_rate": 0.0001918112292482746,
      "loss": 2.7673,
      "step": 440
    },
    {
      "epoch": 0.08394347805810753,
      "grad_norm": 0.16490964591503143,
      "learning_rate": 0.00019162469688490953,
      "loss": 2.7105,
      "step": 450
    },
    {
      "epoch": 0.08580888868162104,
      "grad_norm": 0.14651070535182953,
      "learning_rate": 0.0001914381645215445,
      "loss": 2.6799,
      "step": 460
    },
    {
      "epoch": 0.08767429930513454,
      "grad_norm": 0.19027385115623474,
      "learning_rate": 0.00019125163215817946,
      "loss": 2.7199,
      "step": 470
    },
    {
      "epoch": 0.08953970992864804,
      "grad_norm": 0.18737418949604034,
      "learning_rate": 0.00019106509979481442,
      "loss": 2.7184,
      "step": 480
    },
    {
      "epoch": 0.09140512055216155,
      "grad_norm": 0.1781594455242157,
      "learning_rate": 0.00019087856743144936,
      "loss": 2.705,
      "step": 490
    },
    {
      "epoch": 0.09327053117567505,
      "grad_norm": 0.1659245789051056,
      "learning_rate": 0.00019069203506808432,
      "loss": 2.7077,
      "step": 500
    },
    {
      "epoch": 0.09513594179918855,
      "grad_norm": 0.15972526371479034,
      "learning_rate": 0.00019050550270471926,
      "loss": 2.7109,
      "step": 510
    },
    {
      "epoch": 0.09700135242270205,
      "grad_norm": 0.15085017681121826,
      "learning_rate": 0.00019031897034135425,
      "loss": 2.7615,
      "step": 520
    },
    {
      "epoch": 0.09886676304621554,
      "grad_norm": 0.16208824515342712,
      "learning_rate": 0.0001901324379779892,
      "loss": 2.7671,
      "step": 530
    },
    {
      "epoch": 0.10073217366972904,
      "grad_norm": 0.14619866013526917,
      "learning_rate": 0.00018994590561462416,
      "loss": 2.7531,
      "step": 540
    },
    {
      "epoch": 0.10259758429324255,
      "grad_norm": 0.16509343683719635,
      "learning_rate": 0.0001897593732512591,
      "loss": 2.7066,
      "step": 550
    },
    {
      "epoch": 0.10446299491675605,
      "grad_norm": 0.16182869672775269,
      "learning_rate": 0.00018957284088789406,
      "loss": 2.7984,
      "step": 560
    },
    {
      "epoch": 0.10632840554026955,
      "grad_norm": 0.1429862380027771,
      "learning_rate": 0.00018938630852452902,
      "loss": 2.6908,
      "step": 570
    },
    {
      "epoch": 0.10819381616378305,
      "grad_norm": 0.22219371795654297,
      "learning_rate": 0.000189199776161164,
      "loss": 2.7437,
      "step": 580
    },
    {
      "epoch": 0.11005922678729656,
      "grad_norm": 0.19725380837917328,
      "learning_rate": 0.00018901324379779893,
      "loss": 2.7447,
      "step": 590
    },
    {
      "epoch": 0.11192463741081006,
      "grad_norm": 0.17665508389472961,
      "learning_rate": 0.0001888267114344339,
      "loss": 2.7887,
      "step": 600
    },
    {
      "epoch": 0.11379004803432355,
      "grad_norm": 0.1849178522825241,
      "learning_rate": 0.00018864017907106883,
      "loss": 2.7468,
      "step": 610
    },
    {
      "epoch": 0.11565545865783705,
      "grad_norm": 0.16212716698646545,
      "learning_rate": 0.0001884536467077038,
      "loss": 2.7112,
      "step": 620
    },
    {
      "epoch": 0.11752086928135055,
      "grad_norm": 0.14927330613136292,
      "learning_rate": 0.00018826711434433876,
      "loss": 2.7115,
      "step": 630
    },
    {
      "epoch": 0.11938627990486406,
      "grad_norm": 0.1540161818265915,
      "learning_rate": 0.00018808058198097372,
      "loss": 2.7134,
      "step": 640
    },
    {
      "epoch": 0.12125169052837756,
      "grad_norm": 0.15537329018115997,
      "learning_rate": 0.00018789404961760866,
      "loss": 2.7108,
      "step": 650
    },
    {
      "epoch": 0.12311710115189106,
      "grad_norm": 0.1396607607603073,
      "learning_rate": 0.00018770751725424363,
      "loss": 2.7068,
      "step": 660
    },
    {
      "epoch": 0.12498251177540456,
      "grad_norm": 0.18293245136737823,
      "learning_rate": 0.00018752098489087856,
      "loss": 2.7668,
      "step": 670
    },
    {
      "epoch": 0.12684792239891807,
      "grad_norm": 0.16648982465267181,
      "learning_rate": 0.00018733445252751353,
      "loss": 2.7855,
      "step": 680
    },
    {
      "epoch": 0.12871333302243157,
      "grad_norm": 0.14746910333633423,
      "learning_rate": 0.0001871479201641485,
      "loss": 2.8002,
      "step": 690
    },
    {
      "epoch": 0.13057874364594507,
      "grad_norm": 0.14254461228847504,
      "learning_rate": 0.00018696138780078346,
      "loss": 2.6961,
      "step": 700
    },
    {
      "epoch": 0.13244415426945857,
      "grad_norm": 0.14515061676502228,
      "learning_rate": 0.0001867748554374184,
      "loss": 2.7046,
      "step": 710
    },
    {
      "epoch": 0.13430956489297208,
      "grad_norm": 0.1692812740802765,
      "learning_rate": 0.00018658832307405336,
      "loss": 2.7032,
      "step": 720
    },
    {
      "epoch": 0.13617497551648558,
      "grad_norm": 0.15796712040901184,
      "learning_rate": 0.0001864017907106883,
      "loss": 2.7514,
      "step": 730
    },
    {
      "epoch": 0.13804038613999906,
      "grad_norm": 0.16040121018886566,
      "learning_rate": 0.00018621525834732326,
      "loss": 2.7425,
      "step": 740
    },
    {
      "epoch": 0.13990579676351256,
      "grad_norm": 0.15786580741405487,
      "learning_rate": 0.00018602872598395823,
      "loss": 2.7651,
      "step": 750
    },
    {
      "epoch": 0.14177120738702606,
      "grad_norm": 0.14821821451187134,
      "learning_rate": 0.0001858421936205932,
      "loss": 2.7162,
      "step": 760
    },
    {
      "epoch": 0.14363661801053956,
      "grad_norm": 0.15430599451065063,
      "learning_rate": 0.00018565566125722813,
      "loss": 2.7359,
      "step": 770
    },
    {
      "epoch": 0.14550202863405307,
      "grad_norm": 0.13325853645801544,
      "learning_rate": 0.0001854691288938631,
      "loss": 2.7584,
      "step": 780
    },
    {
      "epoch": 0.14736743925756657,
      "grad_norm": 0.1588333398103714,
      "learning_rate": 0.00018528259653049803,
      "loss": 2.7842,
      "step": 790
    },
    {
      "epoch": 0.14923284988108007,
      "grad_norm": 0.1680953949689865,
      "learning_rate": 0.000185096064167133,
      "loss": 2.7396,
      "step": 800
    },
    {
      "epoch": 0.15109826050459357,
      "grad_norm": 0.18328073620796204,
      "learning_rate": 0.00018490953180376796,
      "loss": 2.7536,
      "step": 810
    },
    {
      "epoch": 0.15296367112810708,
      "grad_norm": 0.1452844738960266,
      "learning_rate": 0.00018472299944040293,
      "loss": 2.7386,
      "step": 820
    },
    {
      "epoch": 0.15482908175162058,
      "grad_norm": 0.13897289335727692,
      "learning_rate": 0.00018453646707703786,
      "loss": 2.716,
      "step": 830
    },
    {
      "epoch": 0.15669449237513408,
      "grad_norm": 0.16417866945266724,
      "learning_rate": 0.00018434993471367283,
      "loss": 2.6755,
      "step": 840
    },
    {
      "epoch": 0.15855990299864758,
      "grad_norm": 0.16245299577713013,
      "learning_rate": 0.00018416340235030777,
      "loss": 2.7261,
      "step": 850
    },
    {
      "epoch": 0.1604253136221611,
      "grad_norm": 0.1654013842344284,
      "learning_rate": 0.00018397686998694276,
      "loss": 2.6809,
      "step": 860
    },
    {
      "epoch": 0.1622907242456746,
      "grad_norm": 0.18488462269306183,
      "learning_rate": 0.0001837903376235777,
      "loss": 2.7033,
      "step": 870
    },
    {
      "epoch": 0.1641561348691881,
      "grad_norm": 0.18236632645130157,
      "learning_rate": 0.00018360380526021266,
      "loss": 2.7749,
      "step": 880
    },
    {
      "epoch": 0.1660215454927016,
      "grad_norm": 0.16065160930156708,
      "learning_rate": 0.0001834172728968476,
      "loss": 2.7067,
      "step": 890
    },
    {
      "epoch": 0.16788695611621507,
      "grad_norm": 0.16159576177597046,
      "learning_rate": 0.00018323074053348256,
      "loss": 2.712,
      "step": 900
    },
    {
      "epoch": 0.16975236673972857,
      "grad_norm": 0.18573586642742157,
      "learning_rate": 0.00018304420817011753,
      "loss": 2.7013,
      "step": 910
    },
    {
      "epoch": 0.17161777736324207,
      "grad_norm": 0.1395568549633026,
      "learning_rate": 0.0001828576758067525,
      "loss": 2.7304,
      "step": 920
    },
    {
      "epoch": 0.17348318798675558,
      "grad_norm": 0.16353273391723633,
      "learning_rate": 0.00018267114344338743,
      "loss": 2.7448,
      "step": 930
    },
    {
      "epoch": 0.17534859861026908,
      "grad_norm": 0.156911700963974,
      "learning_rate": 0.0001824846110800224,
      "loss": 2.7267,
      "step": 940
    },
    {
      "epoch": 0.17721400923378258,
      "grad_norm": 0.14076830446720123,
      "learning_rate": 0.00018229807871665733,
      "loss": 2.7734,
      "step": 950
    },
    {
      "epoch": 0.17907941985729608,
      "grad_norm": 0.24729681015014648,
      "learning_rate": 0.0001821115463532923,
      "loss": 2.7661,
      "step": 960
    },
    {
      "epoch": 0.1809448304808096,
      "grad_norm": 0.16729958355426788,
      "learning_rate": 0.00018192501398992726,
      "loss": 2.7414,
      "step": 970
    },
    {
      "epoch": 0.1828102411043231,
      "grad_norm": 0.16163581609725952,
      "learning_rate": 0.00018173848162656223,
      "loss": 2.7317,
      "step": 980
    },
    {
      "epoch": 0.1846756517278366,
      "grad_norm": 0.16531571745872498,
      "learning_rate": 0.00018155194926319717,
      "loss": 2.7375,
      "step": 990
    },
    {
      "epoch": 0.1865410623513501,
      "grad_norm": 0.147453173995018,
      "learning_rate": 0.00018136541689983213,
      "loss": 2.7406,
      "step": 1000
    },
    {
      "epoch": 0.1884064729748636,
      "grad_norm": 0.2270352840423584,
      "learning_rate": 0.00018117888453646707,
      "loss": 2.7478,
      "step": 1010
    },
    {
      "epoch": 0.1902718835983771,
      "grad_norm": 0.17056028544902802,
      "learning_rate": 0.00018099235217310203,
      "loss": 2.7597,
      "step": 1020
    },
    {
      "epoch": 0.1921372942218906,
      "grad_norm": 0.14907808601856232,
      "learning_rate": 0.000180805819809737,
      "loss": 2.7644,
      "step": 1030
    },
    {
      "epoch": 0.1940027048454041,
      "grad_norm": 0.14189749956130981,
      "learning_rate": 0.00018061928744637196,
      "loss": 2.7268,
      "step": 1040
    },
    {
      "epoch": 0.1958681154689176,
      "grad_norm": 0.17012514173984528,
      "learning_rate": 0.00018043275508300693,
      "loss": 2.748,
      "step": 1050
    },
    {
      "epoch": 0.19773352609243108,
      "grad_norm": 0.16277320683002472,
      "learning_rate": 0.00018024622271964186,
      "loss": 2.7432,
      "step": 1060
    },
    {
      "epoch": 0.19959893671594459,
      "grad_norm": 0.15664391219615936,
      "learning_rate": 0.00018005969035627683,
      "loss": 2.7917,
      "step": 1070
    },
    {
      "epoch": 0.2014643473394581,
      "grad_norm": 0.19512677192687988,
      "learning_rate": 0.00017987315799291177,
      "loss": 2.6948,
      "step": 1080
    },
    {
      "epoch": 0.2033297579629716,
      "grad_norm": 0.1492241770029068,
      "learning_rate": 0.00017968662562954676,
      "loss": 2.717,
      "step": 1090
    },
    {
      "epoch": 0.2051951685864851,
      "grad_norm": 0.15156880021095276,
      "learning_rate": 0.0001795000932661817,
      "loss": 2.7185,
      "step": 1100
    },
    {
      "epoch": 0.2070605792099986,
      "grad_norm": 0.1438760906457901,
      "learning_rate": 0.00017931356090281666,
      "loss": 2.7333,
      "step": 1110
    },
    {
      "epoch": 0.2089259898335121,
      "grad_norm": 0.16253897547721863,
      "learning_rate": 0.0001791270285394516,
      "loss": 2.7371,
      "step": 1120
    },
    {
      "epoch": 0.2107914004570256,
      "grad_norm": 0.16216878592967987,
      "learning_rate": 0.00017894049617608656,
      "loss": 2.692,
      "step": 1130
    },
    {
      "epoch": 0.2126568110805391,
      "grad_norm": 0.1671494096517563,
      "learning_rate": 0.0001787539638127215,
      "loss": 2.7852,
      "step": 1140
    },
    {
      "epoch": 0.2145222217040526,
      "grad_norm": 0.15752249956130981,
      "learning_rate": 0.0001785674314493565,
      "loss": 2.7173,
      "step": 1150
    },
    {
      "epoch": 0.2163876323275661,
      "grad_norm": 0.17554840445518494,
      "learning_rate": 0.00017838089908599143,
      "loss": 2.7462,
      "step": 1160
    },
    {
      "epoch": 0.2182530429510796,
      "grad_norm": 0.15380091965198517,
      "learning_rate": 0.0001781943667226264,
      "loss": 2.7461,
      "step": 1170
    },
    {
      "epoch": 0.22011845357459311,
      "grad_norm": 0.14795756340026855,
      "learning_rate": 0.00017800783435926133,
      "loss": 2.8165,
      "step": 1180
    },
    {
      "epoch": 0.22198386419810662,
      "grad_norm": 0.16866135597229004,
      "learning_rate": 0.0001778213019958963,
      "loss": 2.7415,
      "step": 1190
    },
    {
      "epoch": 0.22384927482162012,
      "grad_norm": 0.14388947188854218,
      "learning_rate": 0.00017763476963253126,
      "loss": 2.7039,
      "step": 1200
    },
    {
      "epoch": 0.22571468544513362,
      "grad_norm": 0.15342052280902863,
      "learning_rate": 0.00017744823726916623,
      "loss": 2.7048,
      "step": 1210
    },
    {
      "epoch": 0.2275800960686471,
      "grad_norm": 0.15989436209201813,
      "learning_rate": 0.00017726170490580117,
      "loss": 2.7106,
      "step": 1220
    },
    {
      "epoch": 0.2294455066921606,
      "grad_norm": 0.1378009021282196,
      "learning_rate": 0.00017707517254243613,
      "loss": 2.7408,
      "step": 1230
    },
    {
      "epoch": 0.2313109173156741,
      "grad_norm": 0.18291008472442627,
      "learning_rate": 0.00017688864017907107,
      "loss": 2.7336,
      "step": 1240
    },
    {
      "epoch": 0.2331763279391876,
      "grad_norm": 0.1433693915605545,
      "learning_rate": 0.00017670210781570603,
      "loss": 2.7048,
      "step": 1250
    },
    {
      "epoch": 0.2350417385627011,
      "grad_norm": 0.14897966384887695,
      "learning_rate": 0.000176515575452341,
      "loss": 2.6999,
      "step": 1260
    },
    {
      "epoch": 0.2369071491862146,
      "grad_norm": 0.17837710678577423,
      "learning_rate": 0.00017632904308897596,
      "loss": 2.7548,
      "step": 1270
    },
    {
      "epoch": 0.2387725598097281,
      "grad_norm": 0.16788427531719208,
      "learning_rate": 0.0001761425107256109,
      "loss": 2.7658,
      "step": 1280
    },
    {
      "epoch": 0.24063797043324162,
      "grad_norm": 0.15667779743671417,
      "learning_rate": 0.00017595597836224586,
      "loss": 2.6949,
      "step": 1290
    },
    {
      "epoch": 0.24250338105675512,
      "grad_norm": 0.14297965168952942,
      "learning_rate": 0.0001757694459988808,
      "loss": 2.7591,
      "step": 1300
    },
    {
      "epoch": 0.24436879168026862,
      "grad_norm": 0.13954482972621918,
      "learning_rate": 0.00017558291363551577,
      "loss": 2.7215,
      "step": 1310
    },
    {
      "epoch": 0.24623420230378212,
      "grad_norm": 0.1571456789970398,
      "learning_rate": 0.00017539638127215073,
      "loss": 2.7778,
      "step": 1320
    },
    {
      "epoch": 0.24809961292729563,
      "grad_norm": 0.1472276747226715,
      "learning_rate": 0.0001752098489087857,
      "loss": 2.7452,
      "step": 1330
    },
    {
      "epoch": 0.24996502355080913,
      "grad_norm": 0.14967314898967743,
      "learning_rate": 0.00017502331654542063,
      "loss": 2.6812,
      "step": 1340
    },
    {
      "epoch": 0.2518304341743226,
      "grad_norm": 0.1614333540201187,
      "learning_rate": 0.0001748367841820556,
      "loss": 2.734,
      "step": 1350
    },
    {
      "epoch": 0.25369584479783613,
      "grad_norm": 0.14609703421592712,
      "learning_rate": 0.00017465025181869054,
      "loss": 2.7345,
      "step": 1360
    },
    {
      "epoch": 0.2555612554213496,
      "grad_norm": 0.14586611092090607,
      "learning_rate": 0.0001744637194553255,
      "loss": 2.6874,
      "step": 1370
    },
    {
      "epoch": 0.25742666604486314,
      "grad_norm": 0.16104786098003387,
      "learning_rate": 0.00017427718709196047,
      "loss": 2.7396,
      "step": 1380
    },
    {
      "epoch": 0.2592920766683766,
      "grad_norm": 0.12530364096164703,
      "learning_rate": 0.00017409065472859543,
      "loss": 2.773,
      "step": 1390
    },
    {
      "epoch": 0.26115748729189014,
      "grad_norm": 0.13997837901115417,
      "learning_rate": 0.00017390412236523037,
      "loss": 2.739,
      "step": 1400
    },
    {
      "epoch": 0.2630228979154036,
      "grad_norm": 0.11734263598918915,
      "learning_rate": 0.00017371759000186533,
      "loss": 2.7345,
      "step": 1410
    },
    {
      "epoch": 0.26488830853891715,
      "grad_norm": 0.1520496904850006,
      "learning_rate": 0.00017353105763850027,
      "loss": 2.72,
      "step": 1420
    },
    {
      "epoch": 0.2667537191624306,
      "grad_norm": 0.12734010815620422,
      "learning_rate": 0.00017334452527513526,
      "loss": 2.7414,
      "step": 1430
    },
    {
      "epoch": 0.26861912978594416,
      "grad_norm": 0.16520115733146667,
      "learning_rate": 0.0001731579929117702,
      "loss": 2.6955,
      "step": 1440
    },
    {
      "epoch": 0.27048454040945763,
      "grad_norm": 0.17836546897888184,
      "learning_rate": 0.00017297146054840517,
      "loss": 2.7515,
      "step": 1450
    },
    {
      "epoch": 0.27234995103297116,
      "grad_norm": 0.1330743134021759,
      "learning_rate": 0.0001727849281850401,
      "loss": 2.7429,
      "step": 1460
    },
    {
      "epoch": 0.27421536165648464,
      "grad_norm": 0.16349083185195923,
      "learning_rate": 0.00017259839582167507,
      "loss": 2.7237,
      "step": 1470
    },
    {
      "epoch": 0.2760807722799981,
      "grad_norm": 0.16724327206611633,
      "learning_rate": 0.00017241186345831,
      "loss": 2.7022,
      "step": 1480
    },
    {
      "epoch": 0.27794618290351164,
      "grad_norm": 0.1447235643863678,
      "learning_rate": 0.000172225331094945,
      "loss": 2.8106,
      "step": 1490
    },
    {
      "epoch": 0.2798115935270251,
      "grad_norm": 0.17385496199131012,
      "learning_rate": 0.00017203879873157994,
      "loss": 2.7174,
      "step": 1500
    },
    {
      "epoch": 0.28167700415053865,
      "grad_norm": 0.15398897230625153,
      "learning_rate": 0.0001718522663682149,
      "loss": 2.7934,
      "step": 1510
    },
    {
      "epoch": 0.2835424147740521,
      "grad_norm": 0.14836399257183075,
      "learning_rate": 0.00017166573400484984,
      "loss": 2.7383,
      "step": 1520
    },
    {
      "epoch": 0.28540782539756565,
      "grad_norm": 0.17768791317939758,
      "learning_rate": 0.0001714792016414848,
      "loss": 2.7429,
      "step": 1530
    },
    {
      "epoch": 0.2872732360210791,
      "grad_norm": 0.17331883311271667,
      "learning_rate": 0.00017129266927811977,
      "loss": 2.7658,
      "step": 1540
    },
    {
      "epoch": 0.28913864664459266,
      "grad_norm": 0.14376869797706604,
      "learning_rate": 0.00017110613691475473,
      "loss": 2.6854,
      "step": 1550
    },
    {
      "epoch": 0.29100405726810613,
      "grad_norm": 0.14987732470035553,
      "learning_rate": 0.00017091960455138967,
      "loss": 2.7346,
      "step": 1560
    },
    {
      "epoch": 0.29286946789161966,
      "grad_norm": 0.14846700429916382,
      "learning_rate": 0.00017073307218802463,
      "loss": 2.7541,
      "step": 1570
    },
    {
      "epoch": 0.29473487851513314,
      "grad_norm": 0.16846182942390442,
      "learning_rate": 0.00017054653982465957,
      "loss": 2.7508,
      "step": 1580
    },
    {
      "epoch": 0.29660028913864667,
      "grad_norm": 0.15939010679721832,
      "learning_rate": 0.00017036000746129454,
      "loss": 2.6678,
      "step": 1590
    },
    {
      "epoch": 0.29846569976216014,
      "grad_norm": 0.18665847182273865,
      "learning_rate": 0.0001701734750979295,
      "loss": 2.7364,
      "step": 1600
    },
    {
      "epoch": 0.30033111038567367,
      "grad_norm": 0.16065096855163574,
      "learning_rate": 0.00016998694273456447,
      "loss": 2.7011,
      "step": 1610
    },
    {
      "epoch": 0.30219652100918715,
      "grad_norm": 0.18118426203727722,
      "learning_rate": 0.0001698004103711994,
      "loss": 2.7706,
      "step": 1620
    },
    {
      "epoch": 0.3040619316327006,
      "grad_norm": 0.20030000805854797,
      "learning_rate": 0.00016961387800783437,
      "loss": 2.707,
      "step": 1630
    },
    {
      "epoch": 0.30592734225621415,
      "grad_norm": 0.14171366393566132,
      "learning_rate": 0.0001694273456444693,
      "loss": 2.7617,
      "step": 1640
    },
    {
      "epoch": 0.3077927528797276,
      "grad_norm": 0.1608070433139801,
      "learning_rate": 0.00016924081328110427,
      "loss": 2.7081,
      "step": 1650
    },
    {
      "epoch": 0.30965816350324116,
      "grad_norm": 0.14796961843967438,
      "learning_rate": 0.00016905428091773924,
      "loss": 2.6924,
      "step": 1660
    },
    {
      "epoch": 0.31152357412675463,
      "grad_norm": 0.14812234044075012,
      "learning_rate": 0.0001688677485543742,
      "loss": 2.6992,
      "step": 1670
    },
    {
      "epoch": 0.31338898475026816,
      "grad_norm": 0.15621592104434967,
      "learning_rate": 0.00016868121619100914,
      "loss": 2.808,
      "step": 1680
    },
    {
      "epoch": 0.31525439537378164,
      "grad_norm": 0.1620316207408905,
      "learning_rate": 0.0001684946838276441,
      "loss": 2.7253,
      "step": 1690
    },
    {
      "epoch": 0.31711980599729517,
      "grad_norm": 0.14455357193946838,
      "learning_rate": 0.00016830815146427904,
      "loss": 2.6783,
      "step": 1700
    },
    {
      "epoch": 0.31898521662080864,
      "grad_norm": 0.155675008893013,
      "learning_rate": 0.000168121619100914,
      "loss": 2.7215,
      "step": 1710
    },
    {
      "epoch": 0.3208506272443222,
      "grad_norm": 0.14709199965000153,
      "learning_rate": 0.00016793508673754897,
      "loss": 2.7295,
      "step": 1720
    },
    {
      "epoch": 0.32271603786783565,
      "grad_norm": 0.1388036608695984,
      "learning_rate": 0.00016774855437418394,
      "loss": 2.7775,
      "step": 1730
    },
    {
      "epoch": 0.3245814484913492,
      "grad_norm": 0.16167998313903809,
      "learning_rate": 0.00016756202201081887,
      "loss": 2.7094,
      "step": 1740
    },
    {
      "epoch": 0.32644685911486265,
      "grad_norm": 0.14482739567756653,
      "learning_rate": 0.00016737548964745384,
      "loss": 2.7489,
      "step": 1750
    },
    {
      "epoch": 0.3283122697383762,
      "grad_norm": 0.1480468213558197,
      "learning_rate": 0.00016718895728408878,
      "loss": 2.7384,
      "step": 1760
    },
    {
      "epoch": 0.33017768036188966,
      "grad_norm": 0.12727978825569153,
      "learning_rate": 0.00016700242492072377,
      "loss": 2.7714,
      "step": 1770
    },
    {
      "epoch": 0.3320430909854032,
      "grad_norm": 0.15377439558506012,
      "learning_rate": 0.0001668158925573587,
      "loss": 2.7371,
      "step": 1780
    },
    {
      "epoch": 0.33390850160891666,
      "grad_norm": 0.15193800628185272,
      "learning_rate": 0.00016662936019399367,
      "loss": 2.7311,
      "step": 1790
    },
    {
      "epoch": 0.33577391223243014,
      "grad_norm": 0.13509348034858704,
      "learning_rate": 0.0001664428278306286,
      "loss": 2.7255,
      "step": 1800
    },
    {
      "epoch": 0.33763932285594367,
      "grad_norm": 0.16201670467853546,
      "learning_rate": 0.00016625629546726357,
      "loss": 2.7499,
      "step": 1810
    },
    {
      "epoch": 0.33950473347945714,
      "grad_norm": 0.1287943571805954,
      "learning_rate": 0.0001660697631038985,
      "loss": 2.6843,
      "step": 1820
    },
    {
      "epoch": 0.3413701441029707,
      "grad_norm": 0.14453931152820587,
      "learning_rate": 0.0001658832307405335,
      "loss": 2.776,
      "step": 1830
    },
    {
      "epoch": 0.34323555472648415,
      "grad_norm": 0.16376110911369324,
      "learning_rate": 0.00016569669837716844,
      "loss": 2.7273,
      "step": 1840
    },
    {
      "epoch": 0.3451009653499977,
      "grad_norm": 0.1452888697385788,
      "learning_rate": 0.0001655101660138034,
      "loss": 2.7495,
      "step": 1850
    },
    {
      "epoch": 0.34696637597351115,
      "grad_norm": 0.15676572918891907,
      "learning_rate": 0.00016532363365043837,
      "loss": 2.7262,
      "step": 1860
    },
    {
      "epoch": 0.3488317865970247,
      "grad_norm": 0.13526572287082672,
      "learning_rate": 0.0001651371012870733,
      "loss": 2.7677,
      "step": 1870
    },
    {
      "epoch": 0.35069719722053816,
      "grad_norm": 0.17894691228866577,
      "learning_rate": 0.00016495056892370827,
      "loss": 2.7833,
      "step": 1880
    },
    {
      "epoch": 0.3525626078440517,
      "grad_norm": 0.19571200013160706,
      "learning_rate": 0.00016476403656034324,
      "loss": 2.744,
      "step": 1890
    },
    {
      "epoch": 0.35442801846756516,
      "grad_norm": 0.1674192249774933,
      "learning_rate": 0.0001645775041969782,
      "loss": 2.7047,
      "step": 1900
    },
    {
      "epoch": 0.3562934290910787,
      "grad_norm": 0.19327156245708466,
      "learning_rate": 0.00016439097183361314,
      "loss": 2.6747,
      "step": 1910
    },
    {
      "epoch": 0.35815883971459217,
      "grad_norm": 0.14167149364948273,
      "learning_rate": 0.0001642044394702481,
      "loss": 2.7039,
      "step": 1920
    },
    {
      "epoch": 0.3600242503381057,
      "grad_norm": 0.14383062720298767,
      "learning_rate": 0.00016401790710688304,
      "loss": 2.7672,
      "step": 1930
    },
    {
      "epoch": 0.3618896609616192,
      "grad_norm": 0.16474440693855286,
      "learning_rate": 0.000163831374743518,
      "loss": 2.7319,
      "step": 1940
    },
    {
      "epoch": 0.36375507158513265,
      "grad_norm": 0.12594258785247803,
      "learning_rate": 0.00016364484238015297,
      "loss": 2.7475,
      "step": 1950
    },
    {
      "epoch": 0.3656204822086462,
      "grad_norm": 0.17379653453826904,
      "learning_rate": 0.00016345831001678794,
      "loss": 2.7486,
      "step": 1960
    },
    {
      "epoch": 0.36748589283215966,
      "grad_norm": 0.1442663073539734,
      "learning_rate": 0.00016327177765342287,
      "loss": 2.7739,
      "step": 1970
    },
    {
      "epoch": 0.3693513034556732,
      "grad_norm": 0.22395439445972443,
      "learning_rate": 0.00016308524529005784,
      "loss": 2.7696,
      "step": 1980
    },
    {
      "epoch": 0.37121671407918666,
      "grad_norm": 0.15733733773231506,
      "learning_rate": 0.00016289871292669278,
      "loss": 2.7415,
      "step": 1990
    },
    {
      "epoch": 0.3730821247027002,
      "grad_norm": 0.18028739094734192,
      "learning_rate": 0.00016271218056332774,
      "loss": 2.7141,
      "step": 2000
    },
    {
      "epoch": 0.37494753532621367,
      "grad_norm": 0.1533733308315277,
      "learning_rate": 0.0001625256481999627,
      "loss": 2.8109,
      "step": 2010
    },
    {
      "epoch": 0.3768129459497272,
      "grad_norm": 0.1375938206911087,
      "learning_rate": 0.00016233911583659767,
      "loss": 2.7428,
      "step": 2020
    },
    {
      "epoch": 0.37867835657324067,
      "grad_norm": 0.14657315611839294,
      "learning_rate": 0.0001621525834732326,
      "loss": 2.6635,
      "step": 2030
    },
    {
      "epoch": 0.3805437671967542,
      "grad_norm": 0.17476943135261536,
      "learning_rate": 0.00016196605110986757,
      "loss": 2.6855,
      "step": 2040
    },
    {
      "epoch": 0.3824091778202677,
      "grad_norm": 0.16908033192157745,
      "learning_rate": 0.0001617795187465025,
      "loss": 2.7138,
      "step": 2050
    },
    {
      "epoch": 0.3842745884437812,
      "grad_norm": 0.17655961215496063,
      "learning_rate": 0.0001615929863831375,
      "loss": 2.7049,
      "step": 2060
    },
    {
      "epoch": 0.3861399990672947,
      "grad_norm": 0.14423628151416779,
      "learning_rate": 0.00016140645401977244,
      "loss": 2.7278,
      "step": 2070
    },
    {
      "epoch": 0.3880054096908082,
      "grad_norm": 0.14817292988300323,
      "learning_rate": 0.0001612199216564074,
      "loss": 2.7356,
      "step": 2080
    },
    {
      "epoch": 0.3898708203143217,
      "grad_norm": 0.15224522352218628,
      "learning_rate": 0.00016103338929304234,
      "loss": 2.7788,
      "step": 2090
    },
    {
      "epoch": 0.3917362309378352,
      "grad_norm": 0.18221932649612427,
      "learning_rate": 0.0001608468569296773,
      "loss": 2.6954,
      "step": 2100
    },
    {
      "epoch": 0.3936016415613487,
      "grad_norm": 0.18721532821655273,
      "learning_rate": 0.00016066032456631227,
      "loss": 2.7212,
      "step": 2110
    },
    {
      "epoch": 0.39546705218486217,
      "grad_norm": 0.15583829581737518,
      "learning_rate": 0.00016047379220294724,
      "loss": 2.7213,
      "step": 2120
    },
    {
      "epoch": 0.3973324628083757,
      "grad_norm": 0.10975374281406403,
      "learning_rate": 0.00016028725983958217,
      "loss": 2.717,
      "step": 2130
    },
    {
      "epoch": 0.39919787343188917,
      "grad_norm": 0.16499724984169006,
      "learning_rate": 0.00016010072747621714,
      "loss": 2.6891,
      "step": 2140
    },
    {
      "epoch": 0.4010632840554027,
      "grad_norm": 0.1480097770690918,
      "learning_rate": 0.00015991419511285208,
      "loss": 2.7504,
      "step": 2150
    },
    {
      "epoch": 0.4029286946789162,
      "grad_norm": 0.1974814087152481,
      "learning_rate": 0.00015972766274948704,
      "loss": 2.7108,
      "step": 2160
    },
    {
      "epoch": 0.4047941053024297,
      "grad_norm": 0.15545015037059784,
      "learning_rate": 0.000159541130386122,
      "loss": 2.7252,
      "step": 2170
    },
    {
      "epoch": 0.4066595159259432,
      "grad_norm": 0.15241597592830658,
      "learning_rate": 0.00015935459802275697,
      "loss": 2.662,
      "step": 2180
    },
    {
      "epoch": 0.4085249265494567,
      "grad_norm": 0.14581938087940216,
      "learning_rate": 0.0001591680656593919,
      "loss": 2.7683,
      "step": 2190
    },
    {
      "epoch": 0.4103903371729702,
      "grad_norm": 0.1427127718925476,
      "learning_rate": 0.00015898153329602687,
      "loss": 2.7672,
      "step": 2200
    },
    {
      "epoch": 0.4122557477964837,
      "grad_norm": 0.21024730801582336,
      "learning_rate": 0.0001587950009326618,
      "loss": 2.6997,
      "step": 2210
    },
    {
      "epoch": 0.4141211584199972,
      "grad_norm": 0.1662793904542923,
      "learning_rate": 0.00015860846856929678,
      "loss": 2.7099,
      "step": 2220
    },
    {
      "epoch": 0.4159865690435107,
      "grad_norm": 0.13693498075008392,
      "learning_rate": 0.00015842193620593174,
      "loss": 2.7399,
      "step": 2230
    },
    {
      "epoch": 0.4178519796670242,
      "grad_norm": 0.1434449851512909,
      "learning_rate": 0.0001582354038425667,
      "loss": 2.7451,
      "step": 2240
    },
    {
      "epoch": 0.41971739029053773,
      "grad_norm": 0.13045243918895721,
      "learning_rate": 0.00015804887147920164,
      "loss": 2.7147,
      "step": 2250
    },
    {
      "epoch": 0.4215828009140512,
      "grad_norm": 0.159250408411026,
      "learning_rate": 0.0001578623391158366,
      "loss": 2.7755,
      "step": 2260
    },
    {
      "epoch": 0.42344821153756473,
      "grad_norm": 0.13985185325145721,
      "learning_rate": 0.00015767580675247155,
      "loss": 2.7623,
      "step": 2270
    },
    {
      "epoch": 0.4253136221610782,
      "grad_norm": 0.15535825490951538,
      "learning_rate": 0.0001574892743891065,
      "loss": 2.7607,
      "step": 2280
    },
    {
      "epoch": 0.4271790327845917,
      "grad_norm": 0.1280566155910492,
      "learning_rate": 0.00015730274202574148,
      "loss": 2.7659,
      "step": 2290
    },
    {
      "epoch": 0.4290444434081052,
      "grad_norm": 0.16127154231071472,
      "learning_rate": 0.00015711620966237644,
      "loss": 2.8047,
      "step": 2300
    },
    {
      "epoch": 0.4309098540316187,
      "grad_norm": 0.15945817530155182,
      "learning_rate": 0.00015692967729901138,
      "loss": 2.7266,
      "step": 2310
    },
    {
      "epoch": 0.4327752646551322,
      "grad_norm": 0.14543235301971436,
      "learning_rate": 0.00015674314493564634,
      "loss": 2.703,
      "step": 2320
    },
    {
      "epoch": 0.4346406752786457,
      "grad_norm": 0.15377601981163025,
      "learning_rate": 0.00015655661257228128,
      "loss": 2.7108,
      "step": 2330
    },
    {
      "epoch": 0.4365060859021592,
      "grad_norm": 0.14333951473236084,
      "learning_rate": 0.00015637008020891625,
      "loss": 2.7219,
      "step": 2340
    },
    {
      "epoch": 0.4383714965256727,
      "grad_norm": 0.16257983446121216,
      "learning_rate": 0.0001561835478455512,
      "loss": 2.7324,
      "step": 2350
    },
    {
      "epoch": 0.44023690714918623,
      "grad_norm": 0.16047506034374237,
      "learning_rate": 0.00015599701548218618,
      "loss": 2.6967,
      "step": 2360
    },
    {
      "epoch": 0.4421023177726997,
      "grad_norm": 0.14970576763153076,
      "learning_rate": 0.0001558104831188211,
      "loss": 2.7145,
      "step": 2370
    },
    {
      "epoch": 0.44396772839621323,
      "grad_norm": 0.16037440299987793,
      "learning_rate": 0.00015562395075545608,
      "loss": 2.7034,
      "step": 2380
    },
    {
      "epoch": 0.4458331390197267,
      "grad_norm": 0.16711211204528809,
      "learning_rate": 0.00015543741839209102,
      "loss": 2.7265,
      "step": 2390
    },
    {
      "epoch": 0.44769854964324024,
      "grad_norm": 0.15029975771903992,
      "learning_rate": 0.000155250886028726,
      "loss": 2.8088,
      "step": 2400
    },
    {
      "epoch": 0.4495639602667537,
      "grad_norm": 0.16391777992248535,
      "learning_rate": 0.00015506435366536094,
      "loss": 2.7385,
      "step": 2410
    },
    {
      "epoch": 0.45142937089026725,
      "grad_norm": 0.13841596245765686,
      "learning_rate": 0.0001548778213019959,
      "loss": 2.7352,
      "step": 2420
    },
    {
      "epoch": 0.4532947815137807,
      "grad_norm": 0.16573286056518555,
      "learning_rate": 0.00015469128893863085,
      "loss": 2.7355,
      "step": 2430
    },
    {
      "epoch": 0.4551601921372942,
      "grad_norm": 0.16358862817287445,
      "learning_rate": 0.0001545047565752658,
      "loss": 2.7088,
      "step": 2440
    },
    {
      "epoch": 0.4570256027608077,
      "grad_norm": 0.12480981647968292,
      "learning_rate": 0.00015431822421190078,
      "loss": 2.6891,
      "step": 2450
    },
    {
      "epoch": 0.4588910133843212,
      "grad_norm": 0.1580915004014969,
      "learning_rate": 0.00015413169184853574,
      "loss": 2.7281,
      "step": 2460
    },
    {
      "epoch": 0.46075642400783473,
      "grad_norm": 0.14165715873241425,
      "learning_rate": 0.00015394515948517068,
      "loss": 2.684,
      "step": 2470
    },
    {
      "epoch": 0.4626218346313482,
      "grad_norm": 0.17354600131511688,
      "learning_rate": 0.00015375862712180564,
      "loss": 2.6869,
      "step": 2480
    },
    {
      "epoch": 0.46448724525486174,
      "grad_norm": 0.17202024161815643,
      "learning_rate": 0.00015357209475844058,
      "loss": 2.7166,
      "step": 2490
    },
    {
      "epoch": 0.4663526558783752,
      "grad_norm": 0.1510486751794815,
      "learning_rate": 0.00015338556239507555,
      "loss": 2.7326,
      "step": 2500
    },
    {
      "epoch": 0.46821806650188874,
      "grad_norm": 0.15641595423221588,
      "learning_rate": 0.0001531990300317105,
      "loss": 2.6962,
      "step": 2510
    },
    {
      "epoch": 0.4700834771254022,
      "grad_norm": 0.1319490671157837,
      "learning_rate": 0.00015301249766834548,
      "loss": 2.7331,
      "step": 2520
    },
    {
      "epoch": 0.47194888774891575,
      "grad_norm": 0.1762932986021042,
      "learning_rate": 0.00015282596530498041,
      "loss": 2.6705,
      "step": 2530
    },
    {
      "epoch": 0.4738142983724292,
      "grad_norm": 0.1698552817106247,
      "learning_rate": 0.00015263943294161538,
      "loss": 2.7097,
      "step": 2540
    },
    {
      "epoch": 0.47567970899594275,
      "grad_norm": 0.16471268236637115,
      "learning_rate": 0.00015245290057825032,
      "loss": 2.7201,
      "step": 2550
    },
    {
      "epoch": 0.4775451196194562,
      "grad_norm": 0.14402230083942413,
      "learning_rate": 0.00015226636821488528,
      "loss": 2.6749,
      "step": 2560
    },
    {
      "epoch": 0.47941053024296976,
      "grad_norm": 0.16145654022693634,
      "learning_rate": 0.00015207983585152025,
      "loss": 2.751,
      "step": 2570
    },
    {
      "epoch": 0.48127594086648323,
      "grad_norm": 0.16775666177272797,
      "learning_rate": 0.0001518933034881552,
      "loss": 2.7591,
      "step": 2580
    },
    {
      "epoch": 0.48314135148999676,
      "grad_norm": 0.1511879712343216,
      "learning_rate": 0.00015170677112479015,
      "loss": 2.7395,
      "step": 2590
    },
    {
      "epoch": 0.48500676211351024,
      "grad_norm": 0.14883340895175934,
      "learning_rate": 0.0001515202387614251,
      "loss": 2.7324,
      "step": 2600
    },
    {
      "epoch": 0.4868721727370237,
      "grad_norm": 0.14826563000679016,
      "learning_rate": 0.00015133370639806005,
      "loss": 2.6861,
      "step": 2610
    },
    {
      "epoch": 0.48873758336053724,
      "grad_norm": 0.14191852509975433,
      "learning_rate": 0.00015114717403469502,
      "loss": 2.7072,
      "step": 2620
    },
    {
      "epoch": 0.4906029939840507,
      "grad_norm": 0.1844250112771988,
      "learning_rate": 0.00015096064167132998,
      "loss": 2.7169,
      "step": 2630
    },
    {
      "epoch": 0.49246840460756425,
      "grad_norm": 0.14169232547283173,
      "learning_rate": 0.00015077410930796495,
      "loss": 2.7453,
      "step": 2640
    },
    {
      "epoch": 0.4943338152310777,
      "grad_norm": 0.14726707339286804,
      "learning_rate": 0.0001505875769445999,
      "loss": 2.7102,
      "step": 2650
    },
    {
      "epoch": 0.49619922585459125,
      "grad_norm": 0.1614682972431183,
      "learning_rate": 0.00015040104458123485,
      "loss": 2.7725,
      "step": 2660
    },
    {
      "epoch": 0.4980646364781047,
      "grad_norm": 0.18527624011039734,
      "learning_rate": 0.0001502145122178698,
      "loss": 2.7687,
      "step": 2670
    },
    {
      "epoch": 0.49993004710161826,
      "grad_norm": 0.19061759114265442,
      "learning_rate": 0.00015002797985450475,
      "loss": 2.7122,
      "step": 2680
    },
    {
      "epoch": 0.5017954577251318,
      "grad_norm": 0.13254684209823608,
      "learning_rate": 0.00014984144749113974,
      "loss": 2.7356,
      "step": 2690
    },
    {
      "epoch": 0.5036608683486452,
      "grad_norm": 0.18172073364257812,
      "learning_rate": 0.00014965491512777468,
      "loss": 2.6931,
      "step": 2700
    },
    {
      "epoch": 0.5055262789721587,
      "grad_norm": 0.16385117173194885,
      "learning_rate": 0.00014946838276440964,
      "loss": 2.7146,
      "step": 2710
    },
    {
      "epoch": 0.5073916895956723,
      "grad_norm": 0.1555185168981552,
      "learning_rate": 0.00014928185040104458,
      "loss": 2.7152,
      "step": 2720
    },
    {
      "epoch": 0.5092571002191858,
      "grad_norm": 0.15423057973384857,
      "learning_rate": 0.00014909531803767955,
      "loss": 2.6844,
      "step": 2730
    },
    {
      "epoch": 0.5111225108426992,
      "grad_norm": 0.16250252723693848,
      "learning_rate": 0.0001489087856743145,
      "loss": 2.7531,
      "step": 2740
    },
    {
      "epoch": 0.5129879214662127,
      "grad_norm": 0.1531093418598175,
      "learning_rate": 0.00014872225331094948,
      "loss": 2.7974,
      "step": 2750
    },
    {
      "epoch": 0.5148533320897263,
      "grad_norm": 0.17549970746040344,
      "learning_rate": 0.00014853572094758441,
      "loss": 2.7255,
      "step": 2760
    },
    {
      "epoch": 0.5167187427132397,
      "grad_norm": 0.1287754774093628,
      "learning_rate": 0.00014834918858421938,
      "loss": 2.7347,
      "step": 2770
    },
    {
      "epoch": 0.5185841533367532,
      "grad_norm": 0.15457911789417267,
      "learning_rate": 0.00014816265622085432,
      "loss": 2.7212,
      "step": 2780
    },
    {
      "epoch": 0.5204495639602668,
      "grad_norm": 0.11603851616382599,
      "learning_rate": 0.00014797612385748928,
      "loss": 2.7677,
      "step": 2790
    },
    {
      "epoch": 0.5223149745837803,
      "grad_norm": 0.1507088989019394,
      "learning_rate": 0.00014778959149412425,
      "loss": 2.6861,
      "step": 2800
    },
    {
      "epoch": 0.5241803852072937,
      "grad_norm": 0.14912645518779755,
      "learning_rate": 0.0001476030591307592,
      "loss": 2.771,
      "step": 2810
    },
    {
      "epoch": 0.5260457958308072,
      "grad_norm": 0.1523490995168686,
      "learning_rate": 0.00014741652676739415,
      "loss": 2.7267,
      "step": 2820
    },
    {
      "epoch": 0.5279112064543208,
      "grad_norm": 0.18948565423488617,
      "learning_rate": 0.00014722999440402911,
      "loss": 2.7016,
      "step": 2830
    },
    {
      "epoch": 0.5297766170778343,
      "grad_norm": 0.1300288885831833,
      "learning_rate": 0.00014704346204066405,
      "loss": 2.7812,
      "step": 2840
    },
    {
      "epoch": 0.5316420277013477,
      "grad_norm": 0.10941703617572784,
      "learning_rate": 0.00014685692967729902,
      "loss": 2.7204,
      "step": 2850
    },
    {
      "epoch": 0.5335074383248612,
      "grad_norm": 0.14700888097286224,
      "learning_rate": 0.00014667039731393398,
      "loss": 2.6822,
      "step": 2860
    },
    {
      "epoch": 0.5353728489483748,
      "grad_norm": 0.1499275416135788,
      "learning_rate": 0.00014648386495056895,
      "loss": 2.7475,
      "step": 2870
    },
    {
      "epoch": 0.5372382595718883,
      "grad_norm": 0.14370925724506378,
      "learning_rate": 0.00014629733258720388,
      "loss": 2.7289,
      "step": 2880
    },
    {
      "epoch": 0.5391036701954017,
      "grad_norm": 0.15946698188781738,
      "learning_rate": 0.00014611080022383885,
      "loss": 2.7701,
      "step": 2890
    },
    {
      "epoch": 0.5409690808189153,
      "grad_norm": 0.1453961580991745,
      "learning_rate": 0.00014592426786047379,
      "loss": 2.7297,
      "step": 2900
    },
    {
      "epoch": 0.5428344914424288,
      "grad_norm": 0.13816212117671967,
      "learning_rate": 0.00014573773549710875,
      "loss": 2.727,
      "step": 2910
    },
    {
      "epoch": 0.5446999020659423,
      "grad_norm": 0.1527823656797409,
      "learning_rate": 0.00014555120313374372,
      "loss": 2.7226,
      "step": 2920
    },
    {
      "epoch": 0.5465653126894557,
      "grad_norm": 0.1478484719991684,
      "learning_rate": 0.00014536467077037868,
      "loss": 2.7657,
      "step": 2930
    },
    {
      "epoch": 0.5484307233129693,
      "grad_norm": 0.16126835346221924,
      "learning_rate": 0.00014517813840701362,
      "loss": 2.718,
      "step": 2940
    },
    {
      "epoch": 0.5502961339364828,
      "grad_norm": 0.13237665593624115,
      "learning_rate": 0.00014499160604364858,
      "loss": 2.7484,
      "step": 2950
    },
    {
      "epoch": 0.5521615445599962,
      "grad_norm": 0.16943292319774628,
      "learning_rate": 0.00014480507368028352,
      "loss": 2.704,
      "step": 2960
    },
    {
      "epoch": 0.5540269551835098,
      "grad_norm": 0.14544665813446045,
      "learning_rate": 0.0001446185413169185,
      "loss": 2.7093,
      "step": 2970
    },
    {
      "epoch": 0.5558923658070233,
      "grad_norm": 0.16921032965183258,
      "learning_rate": 0.00014443200895355345,
      "loss": 2.7305,
      "step": 2980
    },
    {
      "epoch": 0.5577577764305368,
      "grad_norm": 0.1579076498746872,
      "learning_rate": 0.00014424547659018841,
      "loss": 2.7737,
      "step": 2990
    },
    {
      "epoch": 0.5596231870540502,
      "grad_norm": 0.14639298617839813,
      "learning_rate": 0.00014405894422682335,
      "loss": 2.7585,
      "step": 3000
    },
    {
      "epoch": 0.5614885976775638,
      "grad_norm": 0.14550726115703583,
      "learning_rate": 0.00014387241186345832,
      "loss": 2.7364,
      "step": 3010
    },
    {
      "epoch": 0.5633540083010773,
      "grad_norm": 0.15973924100399017,
      "learning_rate": 0.00014368587950009325,
      "loss": 2.7195,
      "step": 3020
    },
    {
      "epoch": 0.5652194189245908,
      "grad_norm": 0.13074544072151184,
      "learning_rate": 0.00014349934713672825,
      "loss": 2.688,
      "step": 3030
    },
    {
      "epoch": 0.5670848295481042,
      "grad_norm": 0.14071820676326752,
      "learning_rate": 0.00014331281477336318,
      "loss": 2.7398,
      "step": 3040
    },
    {
      "epoch": 0.5689502401716178,
      "grad_norm": 0.14068517088890076,
      "learning_rate": 0.00014312628240999815,
      "loss": 2.6956,
      "step": 3050
    },
    {
      "epoch": 0.5708156507951313,
      "grad_norm": 0.11694896966218948,
      "learning_rate": 0.0001429397500466331,
      "loss": 2.7135,
      "step": 3060
    },
    {
      "epoch": 0.5726810614186448,
      "grad_norm": 0.14753791689872742,
      "learning_rate": 0.00014275321768326805,
      "loss": 2.7686,
      "step": 3070
    },
    {
      "epoch": 0.5745464720421583,
      "grad_norm": 0.16397492587566376,
      "learning_rate": 0.00014256668531990302,
      "loss": 2.7841,
      "step": 3080
    },
    {
      "epoch": 0.5764118826656718,
      "grad_norm": 0.16287358105182648,
      "learning_rate": 0.00014238015295653798,
      "loss": 2.8078,
      "step": 3090
    },
    {
      "epoch": 0.5782772932891853,
      "grad_norm": 0.1628761887550354,
      "learning_rate": 0.00014219362059317292,
      "loss": 2.7323,
      "step": 3100
    },
    {
      "epoch": 0.5801427039126987,
      "grad_norm": 0.13463839888572693,
      "learning_rate": 0.00014200708822980788,
      "loss": 2.776,
      "step": 3110
    },
    {
      "epoch": 0.5820081145362123,
      "grad_norm": 0.18159519135951996,
      "learning_rate": 0.00014182055586644282,
      "loss": 2.7013,
      "step": 3120
    },
    {
      "epoch": 0.5838735251597258,
      "grad_norm": 0.1551215946674347,
      "learning_rate": 0.00014163402350307779,
      "loss": 2.7016,
      "step": 3130
    },
    {
      "epoch": 0.5857389357832393,
      "grad_norm": 0.1531403511762619,
      "learning_rate": 0.00014144749113971275,
      "loss": 2.7134,
      "step": 3140
    },
    {
      "epoch": 0.5876043464067527,
      "grad_norm": 0.1504315435886383,
      "learning_rate": 0.00014126095877634772,
      "loss": 2.724,
      "step": 3150
    },
    {
      "epoch": 0.5894697570302663,
      "grad_norm": 0.13867180049419403,
      "learning_rate": 0.00014107442641298265,
      "loss": 2.759,
      "step": 3160
    },
    {
      "epoch": 0.5913351676537798,
      "grad_norm": 0.15521623194217682,
      "learning_rate": 0.00014088789404961762,
      "loss": 2.7673,
      "step": 3170
    },
    {
      "epoch": 0.5932005782772933,
      "grad_norm": 0.13172104954719543,
      "learning_rate": 0.00014070136168625256,
      "loss": 2.7518,
      "step": 3180
    },
    {
      "epoch": 0.5950659889008068,
      "grad_norm": 0.13786308467388153,
      "learning_rate": 0.00014051482932288752,
      "loss": 2.7323,
      "step": 3190
    },
    {
      "epoch": 0.5969313995243203,
      "grad_norm": 0.14392422139644623,
      "learning_rate": 0.00014032829695952249,
      "loss": 2.6894,
      "step": 3200
    },
    {
      "epoch": 0.5987968101478338,
      "grad_norm": 0.15913799405097961,
      "learning_rate": 0.00014014176459615745,
      "loss": 2.7318,
      "step": 3210
    },
    {
      "epoch": 0.6006622207713473,
      "grad_norm": 0.15998901426792145,
      "learning_rate": 0.0001399552322327924,
      "loss": 2.6545,
      "step": 3220
    },
    {
      "epoch": 0.6025276313948608,
      "grad_norm": 0.1073056235909462,
      "learning_rate": 0.00013976869986942735,
      "loss": 2.7356,
      "step": 3230
    },
    {
      "epoch": 0.6043930420183743,
      "grad_norm": 0.1640278846025467,
      "learning_rate": 0.0001395821675060623,
      "loss": 2.7564,
      "step": 3240
    },
    {
      "epoch": 0.6062584526418878,
      "grad_norm": 0.14915530383586884,
      "learning_rate": 0.00013939563514269726,
      "loss": 2.7223,
      "step": 3250
    },
    {
      "epoch": 0.6081238632654012,
      "grad_norm": 0.1492639183998108,
      "learning_rate": 0.00013920910277933222,
      "loss": 2.6989,
      "step": 3260
    },
    {
      "epoch": 0.6099892738889148,
      "grad_norm": 0.1376674920320511,
      "learning_rate": 0.00013902257041596718,
      "loss": 2.7344,
      "step": 3270
    },
    {
      "epoch": 0.6118546845124283,
      "grad_norm": 0.17819467186927795,
      "learning_rate": 0.00013883603805260212,
      "loss": 2.7267,
      "step": 3280
    },
    {
      "epoch": 0.6137200951359418,
      "grad_norm": 0.12982657551765442,
      "learning_rate": 0.0001386495056892371,
      "loss": 2.7353,
      "step": 3290
    },
    {
      "epoch": 0.6155855057594553,
      "grad_norm": 0.1395374834537506,
      "learning_rate": 0.00013846297332587202,
      "loss": 2.7493,
      "step": 3300
    },
    {
      "epoch": 0.6174509163829688,
      "grad_norm": 0.14517004787921906,
      "learning_rate": 0.00013827644096250702,
      "loss": 2.7362,
      "step": 3310
    },
    {
      "epoch": 0.6193163270064823,
      "grad_norm": 0.13376978039741516,
      "learning_rate": 0.00013808990859914195,
      "loss": 2.6779,
      "step": 3320
    },
    {
      "epoch": 0.6211817376299958,
      "grad_norm": 0.14531715214252472,
      "learning_rate": 0.00013790337623577692,
      "loss": 2.7304,
      "step": 3330
    },
    {
      "epoch": 0.6230471482535093,
      "grad_norm": 0.1405872106552124,
      "learning_rate": 0.00013771684387241186,
      "loss": 2.6706,
      "step": 3340
    },
    {
      "epoch": 0.6249125588770228,
      "grad_norm": 0.1414373517036438,
      "learning_rate": 0.00013753031150904682,
      "loss": 2.7057,
      "step": 3350
    },
    {
      "epoch": 0.6267779695005363,
      "grad_norm": 0.1601407825946808,
      "learning_rate": 0.00013734377914568176,
      "loss": 2.7164,
      "step": 3360
    },
    {
      "epoch": 0.6286433801240499,
      "grad_norm": 0.1228860393166542,
      "learning_rate": 0.00013715724678231675,
      "loss": 2.7496,
      "step": 3370
    },
    {
      "epoch": 0.6305087907475633,
      "grad_norm": 0.17435675859451294,
      "learning_rate": 0.0001369707144189517,
      "loss": 2.7728,
      "step": 3380
    },
    {
      "epoch": 0.6323742013710768,
      "grad_norm": 0.18715745210647583,
      "learning_rate": 0.00013678418205558665,
      "loss": 2.7269,
      "step": 3390
    },
    {
      "epoch": 0.6342396119945903,
      "grad_norm": 0.1448206603527069,
      "learning_rate": 0.0001365976496922216,
      "loss": 2.7265,
      "step": 3400
    },
    {
      "epoch": 0.6361050226181038,
      "grad_norm": 0.15800271928310394,
      "learning_rate": 0.00013641111732885656,
      "loss": 2.7216,
      "step": 3410
    },
    {
      "epoch": 0.6379704332416173,
      "grad_norm": 0.15641409158706665,
      "learning_rate": 0.00013622458496549152,
      "loss": 2.7268,
      "step": 3420
    },
    {
      "epoch": 0.6398358438651308,
      "grad_norm": 0.2026728242635727,
      "learning_rate": 0.00013603805260212649,
      "loss": 2.7424,
      "step": 3430
    },
    {
      "epoch": 0.6417012544886443,
      "grad_norm": 0.15961723029613495,
      "learning_rate": 0.00013585152023876142,
      "loss": 2.7527,
      "step": 3440
    },
    {
      "epoch": 0.6435666651121578,
      "grad_norm": 0.15616540610790253,
      "learning_rate": 0.0001356649878753964,
      "loss": 2.6869,
      "step": 3450
    },
    {
      "epoch": 0.6454320757356713,
      "grad_norm": 0.14400804042816162,
      "learning_rate": 0.00013547845551203135,
      "loss": 2.708,
      "step": 3460
    },
    {
      "epoch": 0.6472974863591848,
      "grad_norm": 0.13155406713485718,
      "learning_rate": 0.0001352919231486663,
      "loss": 2.7381,
      "step": 3470
    },
    {
      "epoch": 0.6491628969826984,
      "grad_norm": 0.16130468249320984,
      "learning_rate": 0.00013510539078530126,
      "loss": 2.8241,
      "step": 3480
    },
    {
      "epoch": 0.6510283076062118,
      "grad_norm": 0.16631072759628296,
      "learning_rate": 0.00013491885842193622,
      "loss": 2.6508,
      "step": 3490
    },
    {
      "epoch": 0.6528937182297253,
      "grad_norm": 0.11256881058216095,
      "learning_rate": 0.00013473232605857119,
      "loss": 2.7136,
      "step": 3500
    },
    {
      "epoch": 0.6547591288532388,
      "grad_norm": 0.18147939443588257,
      "learning_rate": 0.00013454579369520612,
      "loss": 2.7682,
      "step": 3510
    },
    {
      "epoch": 0.6566245394767524,
      "grad_norm": 0.15415328741073608,
      "learning_rate": 0.0001343592613318411,
      "loss": 2.7136,
      "step": 3520
    },
    {
      "epoch": 0.6584899501002658,
      "grad_norm": 0.1520017683506012,
      "learning_rate": 0.00013417272896847603,
      "loss": 2.7697,
      "step": 3530
    },
    {
      "epoch": 0.6603553607237793,
      "grad_norm": 0.17101053893566132,
      "learning_rate": 0.000133986196605111,
      "loss": 2.6837,
      "step": 3540
    },
    {
      "epoch": 0.6622207713472928,
      "grad_norm": 0.14572186768054962,
      "learning_rate": 0.00013379966424174595,
      "loss": 2.7062,
      "step": 3550
    },
    {
      "epoch": 0.6640861819708064,
      "grad_norm": 0.1858818084001541,
      "learning_rate": 0.00013361313187838092,
      "loss": 2.7596,
      "step": 3560
    },
    {
      "epoch": 0.6659515925943198,
      "grad_norm": 0.19364392757415771,
      "learning_rate": 0.00013342659951501586,
      "loss": 2.7173,
      "step": 3570
    },
    {
      "epoch": 0.6678170032178333,
      "grad_norm": 0.13730792701244354,
      "learning_rate": 0.00013324006715165082,
      "loss": 2.7117,
      "step": 3580
    },
    {
      "epoch": 0.6696824138413469,
      "grad_norm": 0.12837323546409607,
      "learning_rate": 0.00013305353478828576,
      "loss": 2.6746,
      "step": 3590
    },
    {
      "epoch": 0.6715478244648603,
      "grad_norm": 0.13969843089580536,
      "learning_rate": 0.00013286700242492075,
      "loss": 2.7047,
      "step": 3600
    },
    {
      "epoch": 0.6734132350883738,
      "grad_norm": 0.15722967684268951,
      "learning_rate": 0.0001326804700615557,
      "loss": 2.7198,
      "step": 3610
    },
    {
      "epoch": 0.6752786457118873,
      "grad_norm": 0.1619972437620163,
      "learning_rate": 0.00013249393769819065,
      "loss": 2.7965,
      "step": 3620
    },
    {
      "epoch": 0.6771440563354009,
      "grad_norm": 0.11698783189058304,
      "learning_rate": 0.0001323074053348256,
      "loss": 2.7838,
      "step": 3630
    },
    {
      "epoch": 0.6790094669589143,
      "grad_norm": 0.16048793494701385,
      "learning_rate": 0.00013212087297146056,
      "loss": 2.7389,
      "step": 3640
    },
    {
      "epoch": 0.6808748775824278,
      "grad_norm": 0.15502026677131653,
      "learning_rate": 0.00013193434060809552,
      "loss": 2.7251,
      "step": 3650
    },
    {
      "epoch": 0.6827402882059413,
      "grad_norm": 0.19107453525066376,
      "learning_rate": 0.00013174780824473049,
      "loss": 2.7395,
      "step": 3660
    },
    {
      "epoch": 0.6846056988294549,
      "grad_norm": 0.12281571328639984,
      "learning_rate": 0.00013156127588136542,
      "loss": 2.7626,
      "step": 3670
    },
    {
      "epoch": 0.6864711094529683,
      "grad_norm": 0.12904493510723114,
      "learning_rate": 0.0001313747435180004,
      "loss": 2.7157,
      "step": 3680
    },
    {
      "epoch": 0.6883365200764818,
      "grad_norm": 0.15248794853687286,
      "learning_rate": 0.00013118821115463533,
      "loss": 2.7411,
      "step": 3690
    },
    {
      "epoch": 0.6902019306999954,
      "grad_norm": 0.13914547860622406,
      "learning_rate": 0.0001310016787912703,
      "loss": 2.7252,
      "step": 3700
    },
    {
      "epoch": 0.6920673413235089,
      "grad_norm": 0.1304464191198349,
      "learning_rate": 0.00013081514642790526,
      "loss": 2.7168,
      "step": 3710
    },
    {
      "epoch": 0.6939327519470223,
      "grad_norm": 0.12072426825761795,
      "learning_rate": 0.00013062861406454022,
      "loss": 2.7264,
      "step": 3720
    },
    {
      "epoch": 0.6957981625705358,
      "grad_norm": 0.1241665929555893,
      "learning_rate": 0.00013044208170117516,
      "loss": 2.7439,
      "step": 3730
    },
    {
      "epoch": 0.6976635731940494,
      "grad_norm": 0.11089508980512619,
      "learning_rate": 0.00013025554933781012,
      "loss": 2.7091,
      "step": 3740
    },
    {
      "epoch": 0.6995289838175628,
      "grad_norm": 0.157992884516716,
      "learning_rate": 0.00013006901697444506,
      "loss": 2.7608,
      "step": 3750
    },
    {
      "epoch": 0.7013943944410763,
      "grad_norm": 0.13681907951831818,
      "learning_rate": 0.00012988248461108003,
      "loss": 2.73,
      "step": 3760
    },
    {
      "epoch": 0.7032598050645898,
      "grad_norm": 0.1702084243297577,
      "learning_rate": 0.000129695952247715,
      "loss": 2.7374,
      "step": 3770
    },
    {
      "epoch": 0.7051252156881034,
      "grad_norm": 0.15001298487186432,
      "learning_rate": 0.00012950941988434996,
      "loss": 2.6912,
      "step": 3780
    },
    {
      "epoch": 0.7069906263116168,
      "grad_norm": 0.16615410149097443,
      "learning_rate": 0.0001293228875209849,
      "loss": 2.7353,
      "step": 3790
    },
    {
      "epoch": 0.7088560369351303,
      "grad_norm": 0.14002712070941925,
      "learning_rate": 0.00012913635515761986,
      "loss": 2.6862,
      "step": 3800
    },
    {
      "epoch": 0.7107214475586439,
      "grad_norm": 0.16212747991085052,
      "learning_rate": 0.0001289498227942548,
      "loss": 2.7832,
      "step": 3810
    },
    {
      "epoch": 0.7125868581821574,
      "grad_norm": 0.15552020072937012,
      "learning_rate": 0.00012876329043088976,
      "loss": 2.6769,
      "step": 3820
    },
    {
      "epoch": 0.7144522688056708,
      "grad_norm": 0.15989863872528076,
      "learning_rate": 0.00012857675806752472,
      "loss": 2.7271,
      "step": 3830
    },
    {
      "epoch": 0.7163176794291843,
      "grad_norm": 0.12813739478588104,
      "learning_rate": 0.0001283902257041597,
      "loss": 2.7015,
      "step": 3840
    },
    {
      "epoch": 0.7181830900526979,
      "grad_norm": 0.134817436337471,
      "learning_rate": 0.00012820369334079463,
      "loss": 2.7358,
      "step": 3850
    },
    {
      "epoch": 0.7200485006762114,
      "grad_norm": 0.14282487332820892,
      "learning_rate": 0.0001280171609774296,
      "loss": 2.7272,
      "step": 3860
    },
    {
      "epoch": 0.7219139112997248,
      "grad_norm": 0.16493138670921326,
      "learning_rate": 0.00012783062861406453,
      "loss": 2.7321,
      "step": 3870
    },
    {
      "epoch": 0.7237793219232384,
      "grad_norm": 0.15241286158561707,
      "learning_rate": 0.0001276440962506995,
      "loss": 2.7624,
      "step": 3880
    },
    {
      "epoch": 0.7256447325467519,
      "grad_norm": 0.14600862562656403,
      "learning_rate": 0.00012745756388733446,
      "loss": 2.7078,
      "step": 3890
    },
    {
      "epoch": 0.7275101431702653,
      "grad_norm": 0.15049046277999878,
      "learning_rate": 0.00012727103152396942,
      "loss": 2.7279,
      "step": 3900
    },
    {
      "epoch": 0.7293755537937788,
      "grad_norm": 0.16545265913009644,
      "learning_rate": 0.00012708449916060436,
      "loss": 2.7597,
      "step": 3910
    },
    {
      "epoch": 0.7312409644172924,
      "grad_norm": 0.1576135903596878,
      "learning_rate": 0.00012689796679723933,
      "loss": 2.7341,
      "step": 3920
    },
    {
      "epoch": 0.7331063750408059,
      "grad_norm": 0.16374291479587555,
      "learning_rate": 0.00012671143443387426,
      "loss": 2.6699,
      "step": 3930
    },
    {
      "epoch": 0.7349717856643193,
      "grad_norm": 0.15843285620212555,
      "learning_rate": 0.00012652490207050926,
      "loss": 2.6813,
      "step": 3940
    },
    {
      "epoch": 0.7368371962878328,
      "grad_norm": 0.1512635350227356,
      "learning_rate": 0.0001263383697071442,
      "loss": 2.69,
      "step": 3950
    },
    {
      "epoch": 0.7387026069113464,
      "grad_norm": 0.15473204851150513,
      "learning_rate": 0.00012615183734377916,
      "loss": 2.7745,
      "step": 3960
    },
    {
      "epoch": 0.7405680175348599,
      "grad_norm": 0.13787274062633514,
      "learning_rate": 0.0001259653049804141,
      "loss": 2.7215,
      "step": 3970
    },
    {
      "epoch": 0.7424334281583733,
      "grad_norm": 0.1501370072364807,
      "learning_rate": 0.00012577877261704906,
      "loss": 2.74,
      "step": 3980
    },
    {
      "epoch": 0.7442988387818869,
      "grad_norm": 0.13474008440971375,
      "learning_rate": 0.00012559224025368403,
      "loss": 2.7115,
      "step": 3990
    },
    {
      "epoch": 0.7461642494054004,
      "grad_norm": 0.14711236953735352,
      "learning_rate": 0.000125405707890319,
      "loss": 2.7634,
      "step": 4000
    },
    {
      "epoch": 0.7480296600289139,
      "grad_norm": 0.13509511947631836,
      "learning_rate": 0.00012521917552695393,
      "loss": 2.7501,
      "step": 4010
    },
    {
      "epoch": 0.7498950706524273,
      "grad_norm": 0.14958859980106354,
      "learning_rate": 0.0001250326431635889,
      "loss": 2.7284,
      "step": 4020
    },
    {
      "epoch": 0.7517604812759409,
      "grad_norm": 0.14359673857688904,
      "learning_rate": 0.00012484611080022383,
      "loss": 2.708,
      "step": 4030
    },
    {
      "epoch": 0.7536258918994544,
      "grad_norm": 0.13817550241947174,
      "learning_rate": 0.0001246595784368588,
      "loss": 2.6965,
      "step": 4040
    },
    {
      "epoch": 0.7554913025229679,
      "grad_norm": 0.1577547788619995,
      "learning_rate": 0.00012447304607349376,
      "loss": 2.7223,
      "step": 4050
    },
    {
      "epoch": 0.7573567131464813,
      "grad_norm": 0.15788041055202484,
      "learning_rate": 0.00012428651371012873,
      "loss": 2.7139,
      "step": 4060
    },
    {
      "epoch": 0.7592221237699949,
      "grad_norm": 0.14778774976730347,
      "learning_rate": 0.00012409998134676366,
      "loss": 2.7243,
      "step": 4070
    },
    {
      "epoch": 0.7610875343935084,
      "grad_norm": 0.16646671295166016,
      "learning_rate": 0.00012391344898339863,
      "loss": 2.7372,
      "step": 4080
    },
    {
      "epoch": 0.7629529450170218,
      "grad_norm": 0.18473219871520996,
      "learning_rate": 0.00012372691662003357,
      "loss": 2.7229,
      "step": 4090
    },
    {
      "epoch": 0.7648183556405354,
      "grad_norm": 0.14367955923080444,
      "learning_rate": 0.00012354038425666853,
      "loss": 2.78,
      "step": 4100
    },
    {
      "epoch": 0.7666837662640489,
      "grad_norm": 0.16109991073608398,
      "learning_rate": 0.0001233538518933035,
      "loss": 2.7745,
      "step": 4110
    },
    {
      "epoch": 0.7685491768875624,
      "grad_norm": 0.17410477995872498,
      "learning_rate": 0.00012316731952993846,
      "loss": 2.6958,
      "step": 4120
    },
    {
      "epoch": 0.7704145875110758,
      "grad_norm": 0.12846733629703522,
      "learning_rate": 0.0001229807871665734,
      "loss": 2.7586,
      "step": 4130
    },
    {
      "epoch": 0.7722799981345894,
      "grad_norm": 0.16731008887290955,
      "learning_rate": 0.00012279425480320836,
      "loss": 2.713,
      "step": 4140
    },
    {
      "epoch": 0.7741454087581029,
      "grad_norm": 0.13311943411827087,
      "learning_rate": 0.0001226077224398433,
      "loss": 2.7315,
      "step": 4150
    },
    {
      "epoch": 0.7760108193816164,
      "grad_norm": 0.13564833998680115,
      "learning_rate": 0.00012242119007647826,
      "loss": 2.762,
      "step": 4160
    },
    {
      "epoch": 0.7778762300051298,
      "grad_norm": 0.14259199798107147,
      "learning_rate": 0.00012223465771311323,
      "loss": 2.7608,
      "step": 4170
    },
    {
      "epoch": 0.7797416406286434,
      "grad_norm": 0.1586475372314453,
      "learning_rate": 0.0001220481253497482,
      "loss": 2.7008,
      "step": 4180
    },
    {
      "epoch": 0.7816070512521569,
      "grad_norm": 0.1402914673089981,
      "learning_rate": 0.00012186159298638313,
      "loss": 2.7529,
      "step": 4190
    },
    {
      "epoch": 0.7834724618756704,
      "grad_norm": 0.15219981968402863,
      "learning_rate": 0.0001216750606230181,
      "loss": 2.7572,
      "step": 4200
    },
    {
      "epoch": 0.7853378724991839,
      "grad_norm": 0.17041566967964172,
      "learning_rate": 0.00012148852825965305,
      "loss": 2.7468,
      "step": 4210
    },
    {
      "epoch": 0.7872032831226974,
      "grad_norm": 0.1766139566898346,
      "learning_rate": 0.00012130199589628801,
      "loss": 2.7311,
      "step": 4220
    },
    {
      "epoch": 0.7890686937462109,
      "grad_norm": 0.1475723534822464,
      "learning_rate": 0.00012111546353292295,
      "loss": 2.7163,
      "step": 4230
    },
    {
      "epoch": 0.7909341043697243,
      "grad_norm": 0.1436672806739807,
      "learning_rate": 0.00012092893116955793,
      "loss": 2.7777,
      "step": 4240
    },
    {
      "epoch": 0.7927995149932379,
      "grad_norm": 0.1596381813287735,
      "learning_rate": 0.0001207423988061929,
      "loss": 2.6961,
      "step": 4250
    },
    {
      "epoch": 0.7946649256167514,
      "grad_norm": 0.15521478652954102,
      "learning_rate": 0.00012055586644282783,
      "loss": 2.706,
      "step": 4260
    },
    {
      "epoch": 0.7965303362402649,
      "grad_norm": 0.15300549566745758,
      "learning_rate": 0.00012036933407946281,
      "loss": 2.7263,
      "step": 4270
    },
    {
      "epoch": 0.7983957468637783,
      "grad_norm": 0.17484484612941742,
      "learning_rate": 0.00012018280171609775,
      "loss": 2.6818,
      "step": 4280
    },
    {
      "epoch": 0.8002611574872919,
      "grad_norm": 0.2037094235420227,
      "learning_rate": 0.00011999626935273271,
      "loss": 2.6965,
      "step": 4290
    },
    {
      "epoch": 0.8021265681108054,
      "grad_norm": 0.15169408917427063,
      "learning_rate": 0.00011980973698936766,
      "loss": 2.7074,
      "step": 4300
    },
    {
      "epoch": 0.8039919787343189,
      "grad_norm": 0.15277235209941864,
      "learning_rate": 0.00011962320462600263,
      "loss": 2.7142,
      "step": 4310
    },
    {
      "epoch": 0.8058573893578324,
      "grad_norm": 0.1658749282360077,
      "learning_rate": 0.00011943667226263757,
      "loss": 2.7378,
      "step": 4320
    },
    {
      "epoch": 0.8077227999813459,
      "grad_norm": 0.13132072985172272,
      "learning_rate": 0.00011925013989927254,
      "loss": 2.708,
      "step": 4330
    },
    {
      "epoch": 0.8095882106048594,
      "grad_norm": 0.15243570506572723,
      "learning_rate": 0.00011906360753590748,
      "loss": 2.694,
      "step": 4340
    },
    {
      "epoch": 0.811453621228373,
      "grad_norm": 0.17693616449832916,
      "learning_rate": 0.00011887707517254245,
      "loss": 2.718,
      "step": 4350
    },
    {
      "epoch": 0.8133190318518864,
      "grad_norm": 0.14186488091945648,
      "learning_rate": 0.0001186905428091774,
      "loss": 2.7235,
      "step": 4360
    },
    {
      "epoch": 0.8151844424753999,
      "grad_norm": 0.1659671813249588,
      "learning_rate": 0.00011850401044581236,
      "loss": 2.7086,
      "step": 4370
    },
    {
      "epoch": 0.8170498530989134,
      "grad_norm": 0.15756382048130035,
      "learning_rate": 0.00011831747808244731,
      "loss": 2.714,
      "step": 4380
    },
    {
      "epoch": 0.8189152637224268,
      "grad_norm": 0.1814570277929306,
      "learning_rate": 0.00011813094571908228,
      "loss": 2.793,
      "step": 4390
    },
    {
      "epoch": 0.8207806743459404,
      "grad_norm": 0.1490551382303238,
      "learning_rate": 0.00011794441335571722,
      "loss": 2.7312,
      "step": 4400
    },
    {
      "epoch": 0.8226460849694539,
      "grad_norm": 0.16675341129302979,
      "learning_rate": 0.00011775788099235218,
      "loss": 2.6817,
      "step": 4410
    },
    {
      "epoch": 0.8245114955929674,
      "grad_norm": 0.15103547275066376,
      "learning_rate": 0.00011757134862898713,
      "loss": 2.7411,
      "step": 4420
    },
    {
      "epoch": 0.8263769062164809,
      "grad_norm": 0.1188921183347702,
      "learning_rate": 0.0001173848162656221,
      "loss": 2.7516,
      "step": 4430
    },
    {
      "epoch": 0.8282423168399944,
      "grad_norm": 0.15217839181423187,
      "learning_rate": 0.00011719828390225705,
      "loss": 2.6815,
      "step": 4440
    },
    {
      "epoch": 0.8301077274635079,
      "grad_norm": 0.15394699573516846,
      "learning_rate": 0.00011701175153889201,
      "loss": 2.7377,
      "step": 4450
    },
    {
      "epoch": 0.8319731380870214,
      "grad_norm": 0.13938996195793152,
      "learning_rate": 0.00011682521917552695,
      "loss": 2.8236,
      "step": 4460
    },
    {
      "epoch": 0.8338385487105349,
      "grad_norm": 0.11197708547115326,
      "learning_rate": 0.00011663868681216193,
      "loss": 2.7757,
      "step": 4470
    },
    {
      "epoch": 0.8357039593340484,
      "grad_norm": 0.14300410449504852,
      "learning_rate": 0.00011645215444879687,
      "loss": 2.7542,
      "step": 4480
    },
    {
      "epoch": 0.8375693699575619,
      "grad_norm": 0.14646172523498535,
      "learning_rate": 0.00011626562208543183,
      "loss": 2.6735,
      "step": 4490
    },
    {
      "epoch": 0.8394347805810755,
      "grad_norm": 0.12776701152324677,
      "learning_rate": 0.00011607908972206678,
      "loss": 2.7187,
      "step": 4500
    },
    {
      "epoch": 0.8413001912045889,
      "grad_norm": 0.12227551639080048,
      "learning_rate": 0.00011589255735870175,
      "loss": 2.7769,
      "step": 4510
    },
    {
      "epoch": 0.8431656018281024,
      "grad_norm": 0.1356402337551117,
      "learning_rate": 0.0001157060249953367,
      "loss": 2.7793,
      "step": 4520
    },
    {
      "epoch": 0.8450310124516159,
      "grad_norm": 0.1739988476037979,
      "learning_rate": 0.00011551949263197166,
      "loss": 2.6973,
      "step": 4530
    },
    {
      "epoch": 0.8468964230751295,
      "grad_norm": 0.14886628091335297,
      "learning_rate": 0.0001153329602686066,
      "loss": 2.7272,
      "step": 4540
    },
    {
      "epoch": 0.8487618336986429,
      "grad_norm": 0.1767197698354721,
      "learning_rate": 0.00011514642790524157,
      "loss": 2.7001,
      "step": 4550
    },
    {
      "epoch": 0.8506272443221564,
      "grad_norm": 0.13699930906295776,
      "learning_rate": 0.00011495989554187652,
      "loss": 2.7895,
      "step": 4560
    },
    {
      "epoch": 0.85249265494567,
      "grad_norm": 0.16142690181732178,
      "learning_rate": 0.00011477336317851148,
      "loss": 2.7759,
      "step": 4570
    },
    {
      "epoch": 0.8543580655691834,
      "grad_norm": 0.14382877945899963,
      "learning_rate": 0.00011458683081514643,
      "loss": 2.7335,
      "step": 4580
    },
    {
      "epoch": 0.8562234761926969,
      "grad_norm": 0.16883796453475952,
      "learning_rate": 0.0001144002984517814,
      "loss": 2.7001,
      "step": 4590
    },
    {
      "epoch": 0.8580888868162104,
      "grad_norm": 0.17173951864242554,
      "learning_rate": 0.00011421376608841634,
      "loss": 2.6685,
      "step": 4600
    },
    {
      "epoch": 0.859954297439724,
      "grad_norm": 0.1360911875963211,
      "learning_rate": 0.00011402723372505131,
      "loss": 2.6785,
      "step": 4610
    },
    {
      "epoch": 0.8618197080632374,
      "grad_norm": 0.1528022438287735,
      "learning_rate": 0.00011384070136168625,
      "loss": 2.746,
      "step": 4620
    },
    {
      "epoch": 0.8636851186867509,
      "grad_norm": 0.15242832899093628,
      "learning_rate": 0.00011365416899832122,
      "loss": 2.755,
      "step": 4630
    },
    {
      "epoch": 0.8655505293102644,
      "grad_norm": 0.14116142690181732,
      "learning_rate": 0.00011346763663495617,
      "loss": 2.6901,
      "step": 4640
    },
    {
      "epoch": 0.867415939933778,
      "grad_norm": 0.13596780598163605,
      "learning_rate": 0.00011328110427159113,
      "loss": 2.7597,
      "step": 4650
    },
    {
      "epoch": 0.8692813505572914,
      "grad_norm": 0.13827604055404663,
      "learning_rate": 0.00011309457190822607,
      "loss": 2.7308,
      "step": 4660
    },
    {
      "epoch": 0.8711467611808049,
      "grad_norm": 0.12827251851558685,
      "learning_rate": 0.00011290803954486105,
      "loss": 2.7701,
      "step": 4670
    },
    {
      "epoch": 0.8730121718043184,
      "grad_norm": 0.15511246025562286,
      "learning_rate": 0.00011272150718149599,
      "loss": 2.7194,
      "step": 4680
    },
    {
      "epoch": 0.874877582427832,
      "grad_norm": 0.16866201162338257,
      "learning_rate": 0.00011253497481813095,
      "loss": 2.7317,
      "step": 4690
    },
    {
      "epoch": 0.8767429930513454,
      "grad_norm": 0.15387602150440216,
      "learning_rate": 0.0001123484424547659,
      "loss": 2.6971,
      "step": 4700
    },
    {
      "epoch": 0.8786084036748589,
      "grad_norm": 0.1440751999616623,
      "learning_rate": 0.00011216191009140087,
      "loss": 2.7145,
      "step": 4710
    },
    {
      "epoch": 0.8804738142983725,
      "grad_norm": 0.17137131094932556,
      "learning_rate": 0.00011197537772803582,
      "loss": 2.7196,
      "step": 4720
    },
    {
      "epoch": 0.8823392249218859,
      "grad_norm": 0.13460291922092438,
      "learning_rate": 0.00011178884536467078,
      "loss": 2.7345,
      "step": 4730
    },
    {
      "epoch": 0.8842046355453994,
      "grad_norm": 0.142339825630188,
      "learning_rate": 0.00011160231300130572,
      "loss": 2.7338,
      "step": 4740
    },
    {
      "epoch": 0.8860700461689129,
      "grad_norm": 0.1510322391986847,
      "learning_rate": 0.00011141578063794069,
      "loss": 2.7367,
      "step": 4750
    },
    {
      "epoch": 0.8879354567924265,
      "grad_norm": 0.11825182288885117,
      "learning_rate": 0.00011122924827457564,
      "loss": 2.7535,
      "step": 4760
    },
    {
      "epoch": 0.8898008674159399,
      "grad_norm": 0.14640723168849945,
      "learning_rate": 0.0001110427159112106,
      "loss": 2.7504,
      "step": 4770
    },
    {
      "epoch": 0.8916662780394534,
      "grad_norm": 0.13903437554836273,
      "learning_rate": 0.00011085618354784555,
      "loss": 2.6894,
      "step": 4780
    },
    {
      "epoch": 0.893531688662967,
      "grad_norm": 0.13548089563846588,
      "learning_rate": 0.00011066965118448052,
      "loss": 2.7441,
      "step": 4790
    },
    {
      "epoch": 0.8953970992864805,
      "grad_norm": 0.1313537359237671,
      "learning_rate": 0.00011048311882111546,
      "loss": 2.7267,
      "step": 4800
    },
    {
      "epoch": 0.8972625099099939,
      "grad_norm": 0.16417868435382843,
      "learning_rate": 0.00011029658645775043,
      "loss": 2.7316,
      "step": 4810
    },
    {
      "epoch": 0.8991279205335074,
      "grad_norm": 0.17540878057479858,
      "learning_rate": 0.00011011005409438537,
      "loss": 2.6942,
      "step": 4820
    },
    {
      "epoch": 0.900993331157021,
      "grad_norm": 0.15194207429885864,
      "learning_rate": 0.00010992352173102034,
      "loss": 2.704,
      "step": 4830
    },
    {
      "epoch": 0.9028587417805345,
      "grad_norm": 0.15024544298648834,
      "learning_rate": 0.00010973698936765529,
      "loss": 2.7655,
      "step": 4840
    },
    {
      "epoch": 0.9047241524040479,
      "grad_norm": 0.12804876267910004,
      "learning_rate": 0.00010955045700429025,
      "loss": 2.7213,
      "step": 4850
    },
    {
      "epoch": 0.9065895630275614,
      "grad_norm": 0.14139750599861145,
      "learning_rate": 0.0001093639246409252,
      "loss": 2.7205,
      "step": 4860
    },
    {
      "epoch": 0.908454973651075,
      "grad_norm": 0.15756098926067352,
      "learning_rate": 0.00010917739227756017,
      "loss": 2.6982,
      "step": 4870
    },
    {
      "epoch": 0.9103203842745884,
      "grad_norm": 0.1885260045528412,
      "learning_rate": 0.0001089908599141951,
      "loss": 2.661,
      "step": 4880
    },
    {
      "epoch": 0.9121857948981019,
      "grad_norm": 0.18907701969146729,
      "learning_rate": 0.00010880432755083007,
      "loss": 2.7047,
      "step": 4890
    },
    {
      "epoch": 0.9140512055216155,
      "grad_norm": 0.13436882197856903,
      "learning_rate": 0.00010861779518746502,
      "loss": 2.7105,
      "step": 4900
    },
    {
      "epoch": 0.915916616145129,
      "grad_norm": 0.13333283364772797,
      "learning_rate": 0.00010843126282409999,
      "loss": 2.7034,
      "step": 4910
    },
    {
      "epoch": 0.9177820267686424,
      "grad_norm": 0.12597660720348358,
      "learning_rate": 0.00010824473046073494,
      "loss": 2.7198,
      "step": 4920
    },
    {
      "epoch": 0.9196474373921559,
      "grad_norm": 0.15611658990383148,
      "learning_rate": 0.0001080581980973699,
      "loss": 2.6892,
      "step": 4930
    },
    {
      "epoch": 0.9215128480156695,
      "grad_norm": 0.14951378107070923,
      "learning_rate": 0.00010787166573400484,
      "loss": 2.7322,
      "step": 4940
    },
    {
      "epoch": 0.923378258639183,
      "grad_norm": 0.16599228978157043,
      "learning_rate": 0.00010768513337063982,
      "loss": 2.6964,
      "step": 4950
    },
    {
      "epoch": 0.9252436692626964,
      "grad_norm": 0.14855343103408813,
      "learning_rate": 0.00010749860100727476,
      "loss": 2.7225,
      "step": 4960
    },
    {
      "epoch": 0.9271090798862099,
      "grad_norm": 0.14536283910274506,
      "learning_rate": 0.00010731206864390972,
      "loss": 2.7286,
      "step": 4970
    },
    {
      "epoch": 0.9289744905097235,
      "grad_norm": 0.14560629427433014,
      "learning_rate": 0.00010712553628054467,
      "loss": 2.7588,
      "step": 4980
    },
    {
      "epoch": 0.930839901133237,
      "grad_norm": 0.16418112814426422,
      "learning_rate": 0.00010693900391717964,
      "loss": 2.6925,
      "step": 4990
    },
    {
      "epoch": 0.9327053117567504,
      "grad_norm": 0.12122692167758942,
      "learning_rate": 0.00010675247155381457,
      "loss": 2.7476,
      "step": 5000
    },
    {
      "epoch": 0.934570722380264,
      "grad_norm": 0.12865395843982697,
      "learning_rate": 0.00010656593919044955,
      "loss": 2.7592,
      "step": 5010
    },
    {
      "epoch": 0.9364361330037775,
      "grad_norm": 0.15040405094623566,
      "learning_rate": 0.00010637940682708449,
      "loss": 2.7296,
      "step": 5020
    },
    {
      "epoch": 0.938301543627291,
      "grad_norm": 0.2020762711763382,
      "learning_rate": 0.00010619287446371946,
      "loss": 2.7562,
      "step": 5030
    },
    {
      "epoch": 0.9401669542508044,
      "grad_norm": 0.13188622891902924,
      "learning_rate": 0.00010600634210035441,
      "loss": 2.7547,
      "step": 5040
    },
    {
      "epoch": 0.942032364874318,
      "grad_norm": 0.16048897802829742,
      "learning_rate": 0.00010581980973698937,
      "loss": 2.7471,
      "step": 5050
    },
    {
      "epoch": 0.9438977754978315,
      "grad_norm": 0.1462697833776474,
      "learning_rate": 0.00010563327737362434,
      "loss": 2.7183,
      "step": 5060
    },
    {
      "epoch": 0.9457631861213449,
      "grad_norm": 0.1725383698940277,
      "learning_rate": 0.00010544674501025929,
      "loss": 2.698,
      "step": 5070
    },
    {
      "epoch": 0.9476285967448584,
      "grad_norm": 0.14426307380199432,
      "learning_rate": 0.00010526021264689425,
      "loss": 2.7507,
      "step": 5080
    },
    {
      "epoch": 0.949494007368372,
      "grad_norm": 0.16434916853904724,
      "learning_rate": 0.00010507368028352919,
      "loss": 2.7058,
      "step": 5090
    },
    {
      "epoch": 0.9513594179918855,
      "grad_norm": 0.14627240598201752,
      "learning_rate": 0.00010488714792016417,
      "loss": 2.7293,
      "step": 5100
    },
    {
      "epoch": 0.9532248286153989,
      "grad_norm": 0.2158227413892746,
      "learning_rate": 0.0001047006155567991,
      "loss": 2.7348,
      "step": 5110
    },
    {
      "epoch": 0.9550902392389125,
      "grad_norm": 0.14736498892307281,
      "learning_rate": 0.00010451408319343407,
      "loss": 2.8037,
      "step": 5120
    },
    {
      "epoch": 0.956955649862426,
      "grad_norm": 0.1433393508195877,
      "learning_rate": 0.00010432755083006902,
      "loss": 2.7301,
      "step": 5130
    },
    {
      "epoch": 0.9588210604859395,
      "grad_norm": 0.1695006936788559,
      "learning_rate": 0.00010414101846670399,
      "loss": 2.7802,
      "step": 5140
    },
    {
      "epoch": 0.9606864711094529,
      "grad_norm": 0.1492817997932434,
      "learning_rate": 0.00010395448610333894,
      "loss": 2.6985,
      "step": 5150
    },
    {
      "epoch": 0.9625518817329665,
      "grad_norm": 0.14219918847084045,
      "learning_rate": 0.0001037679537399739,
      "loss": 2.7657,
      "step": 5160
    },
    {
      "epoch": 0.96441729235648,
      "grad_norm": 0.18523430824279785,
      "learning_rate": 0.00010358142137660884,
      "loss": 2.6926,
      "step": 5170
    },
    {
      "epoch": 0.9662827029799935,
      "grad_norm": 0.15116119384765625,
      "learning_rate": 0.0001033948890132438,
      "loss": 2.6795,
      "step": 5180
    },
    {
      "epoch": 0.9681481136035069,
      "grad_norm": 0.15609239041805267,
      "learning_rate": 0.00010320835664987876,
      "loss": 2.7355,
      "step": 5190
    },
    {
      "epoch": 0.9700135242270205,
      "grad_norm": 0.1372271627187729,
      "learning_rate": 0.00010302182428651372,
      "loss": 2.7188,
      "step": 5200
    },
    {
      "epoch": 0.971878934850534,
      "grad_norm": 0.13724812865257263,
      "learning_rate": 0.00010283529192314867,
      "loss": 2.7403,
      "step": 5210
    },
    {
      "epoch": 0.9737443454740474,
      "grad_norm": 0.15410320460796356,
      "learning_rate": 0.00010264875955978364,
      "loss": 2.7179,
      "step": 5220
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 0.14864207804203033,
      "learning_rate": 0.00010246222719641858,
      "loss": 2.7695,
      "step": 5230
    },
    {
      "epoch": 0.9774751667210745,
      "grad_norm": 0.16120699048042297,
      "learning_rate": 0.00010227569483305355,
      "loss": 2.7248,
      "step": 5240
    },
    {
      "epoch": 0.979340577344588,
      "grad_norm": 0.11077997833490372,
      "learning_rate": 0.00010208916246968849,
      "loss": 2.714,
      "step": 5250
    },
    {
      "epoch": 0.9812059879681014,
      "grad_norm": 0.1424986571073532,
      "learning_rate": 0.00010190263010632346,
      "loss": 2.736,
      "step": 5260
    },
    {
      "epoch": 0.983071398591615,
      "grad_norm": 0.15182463824748993,
      "learning_rate": 0.00010171609774295841,
      "loss": 2.6911,
      "step": 5270
    },
    {
      "epoch": 0.9849368092151285,
      "grad_norm": 0.12381912022829056,
      "learning_rate": 0.00010152956537959337,
      "loss": 2.7662,
      "step": 5280
    },
    {
      "epoch": 0.986802219838642,
      "grad_norm": 0.1360470950603485,
      "learning_rate": 0.00010134303301622832,
      "loss": 2.7414,
      "step": 5290
    },
    {
      "epoch": 0.9886676304621554,
      "grad_norm": 0.1554087996482849,
      "learning_rate": 0.00010115650065286329,
      "loss": 2.7346,
      "step": 5300
    },
    {
      "epoch": 0.990533041085669,
      "grad_norm": 0.13190367817878723,
      "learning_rate": 0.00010096996828949823,
      "loss": 2.7242,
      "step": 5310
    },
    {
      "epoch": 0.9923984517091825,
      "grad_norm": 0.16251800954341888,
      "learning_rate": 0.00010078343592613319,
      "loss": 2.702,
      "step": 5320
    },
    {
      "epoch": 0.994263862332696,
      "grad_norm": 0.1599527895450592,
      "learning_rate": 0.00010059690356276814,
      "loss": 2.7298,
      "step": 5330
    },
    {
      "epoch": 0.9961292729562095,
      "grad_norm": 0.13977991044521332,
      "learning_rate": 0.0001004103711994031,
      "loss": 2.7397,
      "step": 5340
    },
    {
      "epoch": 0.997994683579723,
      "grad_norm": 0.20483417809009552,
      "learning_rate": 0.00010022383883603806,
      "loss": 2.717,
      "step": 5350
    },
    {
      "epoch": 0.9998600942032365,
      "grad_norm": 0.13388393819332123,
      "learning_rate": 0.00010003730647267302,
      "loss": 2.722,
      "step": 5360
    },
    {
      "epoch": 1.0016788695611623,
      "grad_norm": 0.15907883644104004,
      "learning_rate": 9.985077410930796e-05,
      "loss": 2.7678,
      "step": 5370
    },
    {
      "epoch": 1.0035442801846757,
      "grad_norm": 0.12614120543003082,
      "learning_rate": 9.966424174594293e-05,
      "loss": 2.7261,
      "step": 5380
    },
    {
      "epoch": 1.005409690808189,
      "grad_norm": 0.15830916166305542,
      "learning_rate": 9.947770938257788e-05,
      "loss": 2.6781,
      "step": 5390
    },
    {
      "epoch": 1.0072751014317027,
      "grad_norm": 0.15470828115940094,
      "learning_rate": 9.929117701921283e-05,
      "loss": 2.7216,
      "step": 5400
    },
    {
      "epoch": 1.0091405120552162,
      "grad_norm": 0.20509105920791626,
      "learning_rate": 9.910464465584779e-05,
      "loss": 2.7666,
      "step": 5410
    },
    {
      "epoch": 1.0110059226787296,
      "grad_norm": 0.1572522670030594,
      "learning_rate": 9.891811229248274e-05,
      "loss": 2.7277,
      "step": 5420
    },
    {
      "epoch": 1.0128713333022432,
      "grad_norm": 0.13531479239463806,
      "learning_rate": 9.87315799291177e-05,
      "loss": 2.6966,
      "step": 5430
    },
    {
      "epoch": 1.0147367439257566,
      "grad_norm": 0.13627031445503235,
      "learning_rate": 9.854504756575266e-05,
      "loss": 2.6763,
      "step": 5440
    },
    {
      "epoch": 1.01660215454927,
      "grad_norm": 0.12906251847743988,
      "learning_rate": 9.835851520238762e-05,
      "loss": 2.7338,
      "step": 5450
    },
    {
      "epoch": 1.0184675651727837,
      "grad_norm": 0.1361578106880188,
      "learning_rate": 9.817198283902258e-05,
      "loss": 2.703,
      "step": 5460
    },
    {
      "epoch": 1.0203329757962971,
      "grad_norm": 0.1624879091978073,
      "learning_rate": 9.798545047565754e-05,
      "loss": 2.7382,
      "step": 5470
    },
    {
      "epoch": 1.0221983864198108,
      "grad_norm": 0.11892204731702805,
      "learning_rate": 9.779891811229249e-05,
      "loss": 2.7288,
      "step": 5480
    },
    {
      "epoch": 1.0240637970433242,
      "grad_norm": 0.14516963064670563,
      "learning_rate": 9.761238574892744e-05,
      "loss": 2.6676,
      "step": 5490
    },
    {
      "epoch": 1.0259292076668376,
      "grad_norm": 0.12808756530284882,
      "learning_rate": 9.742585338556241e-05,
      "loss": 2.692,
      "step": 5500
    },
    {
      "epoch": 1.0277946182903512,
      "grad_norm": 0.17845305800437927,
      "learning_rate": 9.723932102219736e-05,
      "loss": 2.6797,
      "step": 5510
    },
    {
      "epoch": 1.0296600289138647,
      "grad_norm": 0.17532891035079956,
      "learning_rate": 9.705278865883231e-05,
      "loss": 2.7613,
      "step": 5520
    },
    {
      "epoch": 1.031525439537378,
      "grad_norm": 0.16771651804447174,
      "learning_rate": 9.686625629546727e-05,
      "loss": 2.6538,
      "step": 5530
    },
    {
      "epoch": 1.0333908501608917,
      "grad_norm": 0.14253786206245422,
      "learning_rate": 9.667972393210223e-05,
      "loss": 2.7537,
      "step": 5540
    },
    {
      "epoch": 1.0352562607844051,
      "grad_norm": 0.17185771465301514,
      "learning_rate": 9.649319156873719e-05,
      "loss": 2.6778,
      "step": 5550
    },
    {
      "epoch": 1.0371216714079186,
      "grad_norm": 0.1648075431585312,
      "learning_rate": 9.630665920537214e-05,
      "loss": 2.7866,
      "step": 5560
    },
    {
      "epoch": 1.0389870820314322,
      "grad_norm": 0.15466317534446716,
      "learning_rate": 9.61201268420071e-05,
      "loss": 2.6942,
      "step": 5570
    },
    {
      "epoch": 1.0408524926549456,
      "grad_norm": 0.1514068990945816,
      "learning_rate": 9.593359447864206e-05,
      "loss": 2.7162,
      "step": 5580
    },
    {
      "epoch": 1.0427179032784593,
      "grad_norm": 0.1424640566110611,
      "learning_rate": 9.574706211527701e-05,
      "loss": 2.7167,
      "step": 5590
    },
    {
      "epoch": 1.0445833139019727,
      "grad_norm": 0.15159901976585388,
      "learning_rate": 9.556052975191196e-05,
      "loss": 2.6902,
      "step": 5600
    },
    {
      "epoch": 1.046448724525486,
      "grad_norm": 0.16549180448055267,
      "learning_rate": 9.537399738854693e-05,
      "loss": 2.7531,
      "step": 5610
    },
    {
      "epoch": 1.0483141351489997,
      "grad_norm": 0.16237327456474304,
      "learning_rate": 9.518746502518188e-05,
      "loss": 2.7205,
      "step": 5620
    },
    {
      "epoch": 1.0501795457725132,
      "grad_norm": 0.13696187734603882,
      "learning_rate": 9.500093266181683e-05,
      "loss": 2.7147,
      "step": 5630
    },
    {
      "epoch": 1.0520449563960266,
      "grad_norm": 0.15274731814861298,
      "learning_rate": 9.481440029845179e-05,
      "loss": 2.6794,
      "step": 5640
    },
    {
      "epoch": 1.0539103670195402,
      "grad_norm": 0.15651114284992218,
      "learning_rate": 9.462786793508674e-05,
      "loss": 2.751,
      "step": 5650
    },
    {
      "epoch": 1.0557757776430536,
      "grad_norm": 0.1577000916004181,
      "learning_rate": 9.44413355717217e-05,
      "loss": 2.7041,
      "step": 5660
    },
    {
      "epoch": 1.0576411882665673,
      "grad_norm": 0.16378410160541534,
      "learning_rate": 9.425480320835666e-05,
      "loss": 2.6661,
      "step": 5670
    },
    {
      "epoch": 1.0595065988900807,
      "grad_norm": 0.14660510420799255,
      "learning_rate": 9.406827084499161e-05,
      "loss": 2.718,
      "step": 5680
    },
    {
      "epoch": 1.0613720095135941,
      "grad_norm": 0.16414859890937805,
      "learning_rate": 9.388173848162656e-05,
      "loss": 2.689,
      "step": 5690
    },
    {
      "epoch": 1.0632374201371078,
      "grad_norm": 0.13056425750255585,
      "learning_rate": 9.369520611826153e-05,
      "loss": 2.6994,
      "step": 5700
    },
    {
      "epoch": 1.0651028307606212,
      "grad_norm": 0.1397065967321396,
      "learning_rate": 9.350867375489648e-05,
      "loss": 2.7669,
      "step": 5710
    },
    {
      "epoch": 1.0669682413841346,
      "grad_norm": 0.17218270897865295,
      "learning_rate": 9.332214139153144e-05,
      "loss": 2.7085,
      "step": 5720
    },
    {
      "epoch": 1.0688336520076482,
      "grad_norm": 0.10984966903924942,
      "learning_rate": 9.31356090281664e-05,
      "loss": 2.681,
      "step": 5730
    },
    {
      "epoch": 1.0706990626311617,
      "grad_norm": 0.17977800965309143,
      "learning_rate": 9.294907666480135e-05,
      "loss": 2.6775,
      "step": 5740
    },
    {
      "epoch": 1.0725644732546753,
      "grad_norm": 0.22587311267852783,
      "learning_rate": 9.276254430143631e-05,
      "loss": 2.6849,
      "step": 5750
    },
    {
      "epoch": 1.0744298838781887,
      "grad_norm": 0.1684294492006302,
      "learning_rate": 9.257601193807126e-05,
      "loss": 2.7832,
      "step": 5760
    },
    {
      "epoch": 1.0762952945017021,
      "grad_norm": 0.14221680164337158,
      "learning_rate": 9.238947957470621e-05,
      "loss": 2.7409,
      "step": 5770
    },
    {
      "epoch": 1.0781607051252158,
      "grad_norm": 0.1502632051706314,
      "learning_rate": 9.220294721134118e-05,
      "loss": 2.763,
      "step": 5780
    },
    {
      "epoch": 1.0800261157487292,
      "grad_norm": 0.1842549443244934,
      "learning_rate": 9.201641484797613e-05,
      "loss": 2.7691,
      "step": 5790
    },
    {
      "epoch": 1.0818915263722426,
      "grad_norm": 0.12485894560813904,
      "learning_rate": 9.182988248461108e-05,
      "loss": 2.7286,
      "step": 5800
    },
    {
      "epoch": 1.0837569369957563,
      "grad_norm": 0.14307215809822083,
      "learning_rate": 9.164335012124605e-05,
      "loss": 2.6746,
      "step": 5810
    },
    {
      "epoch": 1.0856223476192697,
      "grad_norm": 0.15195858478546143,
      "learning_rate": 9.1456817757881e-05,
      "loss": 2.7699,
      "step": 5820
    },
    {
      "epoch": 1.087487758242783,
      "grad_norm": 0.14557483792304993,
      "learning_rate": 9.127028539451595e-05,
      "loss": 2.7583,
      "step": 5830
    },
    {
      "epoch": 1.0893531688662967,
      "grad_norm": 0.13773325085639954,
      "learning_rate": 9.108375303115091e-05,
      "loss": 2.7568,
      "step": 5840
    },
    {
      "epoch": 1.0912185794898102,
      "grad_norm": 0.15085247159004211,
      "learning_rate": 9.089722066778586e-05,
      "loss": 2.7762,
      "step": 5850
    },
    {
      "epoch": 1.0930839901133238,
      "grad_norm": 0.16394516825675964,
      "learning_rate": 9.071068830442081e-05,
      "loss": 2.6881,
      "step": 5860
    },
    {
      "epoch": 1.0949494007368372,
      "grad_norm": 0.13580180704593658,
      "learning_rate": 9.052415594105578e-05,
      "loss": 2.7001,
      "step": 5870
    },
    {
      "epoch": 1.0968148113603506,
      "grad_norm": 0.1719682365655899,
      "learning_rate": 9.033762357769073e-05,
      "loss": 2.7937,
      "step": 5880
    },
    {
      "epoch": 1.0986802219838643,
      "grad_norm": 0.17207007110118866,
      "learning_rate": 9.01510912143257e-05,
      "loss": 2.659,
      "step": 5890
    },
    {
      "epoch": 1.1005456326073777,
      "grad_norm": 0.14990130066871643,
      "learning_rate": 8.996455885096065e-05,
      "loss": 2.7184,
      "step": 5900
    },
    {
      "epoch": 1.1024110432308911,
      "grad_norm": 0.15459361672401428,
      "learning_rate": 8.97780264875956e-05,
      "loss": 2.7324,
      "step": 5910
    },
    {
      "epoch": 1.1042764538544048,
      "grad_norm": 0.16569337248802185,
      "learning_rate": 8.959149412423056e-05,
      "loss": 2.6596,
      "step": 5920
    },
    {
      "epoch": 1.1061418644779182,
      "grad_norm": 0.14923380315303802,
      "learning_rate": 8.940496176086551e-05,
      "loss": 2.6864,
      "step": 5930
    },
    {
      "epoch": 1.1080072751014316,
      "grad_norm": 0.15956972539424896,
      "learning_rate": 8.921842939750047e-05,
      "loss": 2.7263,
      "step": 5940
    },
    {
      "epoch": 1.1098726857249452,
      "grad_norm": 0.150583878159523,
      "learning_rate": 8.903189703413543e-05,
      "loss": 2.7125,
      "step": 5950
    },
    {
      "epoch": 1.1117380963484587,
      "grad_norm": 0.14163236320018768,
      "learning_rate": 8.884536467077038e-05,
      "loss": 2.7346,
      "step": 5960
    },
    {
      "epoch": 1.1136035069719723,
      "grad_norm": 0.13363829255104065,
      "learning_rate": 8.865883230740533e-05,
      "loss": 2.7135,
      "step": 5970
    },
    {
      "epoch": 1.1154689175954857,
      "grad_norm": 0.16436618566513062,
      "learning_rate": 8.84722999440403e-05,
      "loss": 2.658,
      "step": 5980
    },
    {
      "epoch": 1.1173343282189991,
      "grad_norm": 0.16291511058807373,
      "learning_rate": 8.828576758067525e-05,
      "loss": 2.6754,
      "step": 5990
    },
    {
      "epoch": 1.1191997388425128,
      "grad_norm": 0.15667350590229034,
      "learning_rate": 8.80992352173102e-05,
      "loss": 2.6674,
      "step": 6000
    },
    {
      "epoch": 1.1210651494660262,
      "grad_norm": 0.18400156497955322,
      "learning_rate": 8.791270285394516e-05,
      "loss": 2.691,
      "step": 6010
    },
    {
      "epoch": 1.1229305600895396,
      "grad_norm": 0.15790311992168427,
      "learning_rate": 8.772617049058012e-05,
      "loss": 2.6919,
      "step": 6020
    },
    {
      "epoch": 1.1247959707130533,
      "grad_norm": 0.13715165853500366,
      "learning_rate": 8.753963812721507e-05,
      "loss": 2.7067,
      "step": 6030
    },
    {
      "epoch": 1.1266613813365667,
      "grad_norm": 0.1505572348833084,
      "learning_rate": 8.735310576385003e-05,
      "loss": 2.6945,
      "step": 6040
    },
    {
      "epoch": 1.12852679196008,
      "grad_norm": 0.15903088450431824,
      "learning_rate": 8.716657340048498e-05,
      "loss": 2.6731,
      "step": 6050
    },
    {
      "epoch": 1.1303922025835937,
      "grad_norm": 0.15620696544647217,
      "learning_rate": 8.698004103711995e-05,
      "loss": 2.7264,
      "step": 6060
    },
    {
      "epoch": 1.1322576132071072,
      "grad_norm": 0.17517592012882233,
      "learning_rate": 8.67935086737549e-05,
      "loss": 2.7418,
      "step": 6070
    },
    {
      "epoch": 1.1341230238306208,
      "grad_norm": 0.16449034214019775,
      "learning_rate": 8.660697631038985e-05,
      "loss": 2.7679,
      "step": 6080
    },
    {
      "epoch": 1.1359884344541342,
      "grad_norm": 0.1675075888633728,
      "learning_rate": 8.642044394702482e-05,
      "loss": 2.7208,
      "step": 6090
    },
    {
      "epoch": 1.1378538450776476,
      "grad_norm": 0.134971484541893,
      "learning_rate": 8.623391158365977e-05,
      "loss": 2.7443,
      "step": 6100
    },
    {
      "epoch": 1.1397192557011613,
      "grad_norm": 0.22195343673229218,
      "learning_rate": 8.604737922029472e-05,
      "loss": 2.7177,
      "step": 6110
    },
    {
      "epoch": 1.1415846663246747,
      "grad_norm": 0.15914024412631989,
      "learning_rate": 8.586084685692968e-05,
      "loss": 2.7707,
      "step": 6120
    },
    {
      "epoch": 1.1434500769481883,
      "grad_norm": 0.1546802967786789,
      "learning_rate": 8.567431449356463e-05,
      "loss": 2.6901,
      "step": 6130
    },
    {
      "epoch": 1.1453154875717018,
      "grad_norm": 0.1559315174818039,
      "learning_rate": 8.548778213019958e-05,
      "loss": 2.7432,
      "step": 6140
    },
    {
      "epoch": 1.1471808981952152,
      "grad_norm": 0.14780907332897186,
      "learning_rate": 8.530124976683455e-05,
      "loss": 2.6345,
      "step": 6150
    },
    {
      "epoch": 1.1490463088187286,
      "grad_norm": 0.12434403598308563,
      "learning_rate": 8.51147174034695e-05,
      "loss": 2.7743,
      "step": 6160
    },
    {
      "epoch": 1.1509117194422422,
      "grad_norm": 0.13110202550888062,
      "learning_rate": 8.492818504010445e-05,
      "loss": 2.7176,
      "step": 6170
    },
    {
      "epoch": 1.1527771300657557,
      "grad_norm": 0.13907350599765778,
      "learning_rate": 8.474165267673942e-05,
      "loss": 2.8209,
      "step": 6180
    },
    {
      "epoch": 1.1546425406892693,
      "grad_norm": 0.14020051062107086,
      "learning_rate": 8.455512031337437e-05,
      "loss": 2.728,
      "step": 6190
    },
    {
      "epoch": 1.1565079513127827,
      "grad_norm": 0.15491074323654175,
      "learning_rate": 8.436858795000932e-05,
      "loss": 2.7163,
      "step": 6200
    },
    {
      "epoch": 1.1583733619362961,
      "grad_norm": 0.14317673444747925,
      "learning_rate": 8.418205558664428e-05,
      "loss": 2.6949,
      "step": 6210
    },
    {
      "epoch": 1.1602387725598098,
      "grad_norm": 0.1433391571044922,
      "learning_rate": 8.399552322327924e-05,
      "loss": 2.6794,
      "step": 6220
    },
    {
      "epoch": 1.1621041831833232,
      "grad_norm": 0.14795029163360596,
      "learning_rate": 8.38089908599142e-05,
      "loss": 2.6952,
      "step": 6230
    },
    {
      "epoch": 1.1639695938068368,
      "grad_norm": 0.14428868889808655,
      "learning_rate": 8.362245849654915e-05,
      "loss": 2.7168,
      "step": 6240
    },
    {
      "epoch": 1.1658350044303503,
      "grad_norm": 0.18727324903011322,
      "learning_rate": 8.343592613318412e-05,
      "loss": 2.6901,
      "step": 6250
    },
    {
      "epoch": 1.1677004150538637,
      "grad_norm": 0.15149667859077454,
      "learning_rate": 8.324939376981907e-05,
      "loss": 2.7239,
      "step": 6260
    },
    {
      "epoch": 1.1695658256773773,
      "grad_norm": 0.15298254787921906,
      "learning_rate": 8.306286140645403e-05,
      "loss": 2.7634,
      "step": 6270
    },
    {
      "epoch": 1.1714312363008907,
      "grad_norm": 0.2094229757785797,
      "learning_rate": 8.287632904308898e-05,
      "loss": 2.7092,
      "step": 6280
    },
    {
      "epoch": 1.1732966469244042,
      "grad_norm": 0.14715667068958282,
      "learning_rate": 8.268979667972393e-05,
      "loss": 2.7261,
      "step": 6290
    },
    {
      "epoch": 1.1751620575479178,
      "grad_norm": 0.16331900656223297,
      "learning_rate": 8.25032643163589e-05,
      "loss": 2.663,
      "step": 6300
    },
    {
      "epoch": 1.1770274681714312,
      "grad_norm": 0.1427738517522812,
      "learning_rate": 8.231673195299385e-05,
      "loss": 2.6972,
      "step": 6310
    },
    {
      "epoch": 1.1788928787949446,
      "grad_norm": 0.14186806976795197,
      "learning_rate": 8.213019958962882e-05,
      "loss": 2.7501,
      "step": 6320
    },
    {
      "epoch": 1.1807582894184583,
      "grad_norm": 0.15916772186756134,
      "learning_rate": 8.194366722626377e-05,
      "loss": 2.729,
      "step": 6330
    },
    {
      "epoch": 1.1826237000419717,
      "grad_norm": 0.15342923998832703,
      "learning_rate": 8.175713486289872e-05,
      "loss": 2.6978,
      "step": 6340
    },
    {
      "epoch": 1.1844891106654853,
      "grad_norm": 0.12934790551662445,
      "learning_rate": 8.157060249953368e-05,
      "loss": 2.7482,
      "step": 6350
    },
    {
      "epoch": 1.1863545212889988,
      "grad_norm": 0.12227218598127365,
      "learning_rate": 8.138407013616863e-05,
      "loss": 2.7495,
      "step": 6360
    },
    {
      "epoch": 1.1882199319125122,
      "grad_norm": 0.14219766855239868,
      "learning_rate": 8.119753777280359e-05,
      "loss": 2.6882,
      "step": 6370
    },
    {
      "epoch": 1.1900853425360258,
      "grad_norm": 0.15274563431739807,
      "learning_rate": 8.101100540943855e-05,
      "loss": 2.6892,
      "step": 6380
    },
    {
      "epoch": 1.1919507531595392,
      "grad_norm": 0.2171514928340912,
      "learning_rate": 8.08244730460735e-05,
      "loss": 2.7739,
      "step": 6390
    },
    {
      "epoch": 1.1938161637830527,
      "grad_norm": 0.15112274885177612,
      "learning_rate": 8.063794068270845e-05,
      "loss": 2.7193,
      "step": 6400
    },
    {
      "epoch": 1.1956815744065663,
      "grad_norm": 0.15741848945617676,
      "learning_rate": 8.045140831934342e-05,
      "loss": 2.7347,
      "step": 6410
    },
    {
      "epoch": 1.1975469850300797,
      "grad_norm": 0.16311468183994293,
      "learning_rate": 8.026487595597837e-05,
      "loss": 2.6683,
      "step": 6420
    },
    {
      "epoch": 1.1994123956535931,
      "grad_norm": 0.16270612180233002,
      "learning_rate": 8.007834359261332e-05,
      "loss": 2.742,
      "step": 6430
    },
    {
      "epoch": 1.2012778062771068,
      "grad_norm": 0.149488165974617,
      "learning_rate": 7.989181122924828e-05,
      "loss": 2.6928,
      "step": 6440
    },
    {
      "epoch": 1.2031432169006202,
      "grad_norm": 0.1639094054698944,
      "learning_rate": 7.970527886588324e-05,
      "loss": 2.7569,
      "step": 6450
    },
    {
      "epoch": 1.2050086275241338,
      "grad_norm": 0.15015944838523865,
      "learning_rate": 7.951874650251819e-05,
      "loss": 2.6984,
      "step": 6460
    },
    {
      "epoch": 1.2068740381476473,
      "grad_norm": 0.17190362513065338,
      "learning_rate": 7.933221413915315e-05,
      "loss": 2.6825,
      "step": 6470
    },
    {
      "epoch": 1.2087394487711607,
      "grad_norm": 0.1531849354505539,
      "learning_rate": 7.91456817757881e-05,
      "loss": 2.7071,
      "step": 6480
    },
    {
      "epoch": 1.2106048593946743,
      "grad_norm": 0.14635340869426727,
      "learning_rate": 7.895914941242307e-05,
      "loss": 2.7202,
      "step": 6490
    },
    {
      "epoch": 1.2124702700181877,
      "grad_norm": 0.1645113229751587,
      "learning_rate": 7.877261704905802e-05,
      "loss": 2.7533,
      "step": 6500
    }
  ],
  "logging_steps": 10,
  "max_steps": 10722,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 9.36580301656105e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
