{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T18:33:50.373030Z",
     "iopub.status.busy": "2025-12-21T18:33:50.372612Z",
     "iopub.status.idle": "2025-12-21T18:34:02.443743Z",
     "shell.execute_reply": "2025-12-21T18:34:02.442694Z",
     "shell.execute_reply.started": "2025-12-21T18:33:50.373000Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y transformers peft tokenizers accelerate\n",
    "!pip install transformers==4.38.2 peft==0.8.2 accelerate==0.27.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T18:46:56.841935Z",
     "iopub.status.busy": "2025-12-21T18:46:56.841102Z",
     "iopub.status.idle": "2025-12-21T18:47:16.149998Z",
     "shell.execute_reply": "2025-12-21T18:47:16.149271Z",
     "shell.execute_reply.started": "2025-12-21T18:46:56.841902Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade accelerate peft transformers datasets pillow huggingface_hub --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T18:48:45.224079Z",
     "iopub.status.busy": "2025-12-21T18:48:45.223294Z",
     "iopub.status.idle": "2025-12-21T18:48:58.407344Z",
     "shell.execute_reply": "2025-12-21T18:48:58.406589Z",
     "shell.execute_reply.started": "2025-12-21T18:48:45.224036Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade git+https://github.com/haotian-liu/LLaVA.git\n",
    "!pip install transformers accelerate datasets safetensors peft bitsandbytes sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T18:53:25.067635Z",
     "iopub.status.busy": "2025-12-21T18:53:25.067092Z",
     "iopub.status.idle": "2025-12-21T18:57:23.665102Z",
     "shell.execute_reply": "2025-12-21T18:57:23.664422Z",
     "shell.execute_reply.started": "2025-12-21T18:53:25.067606Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Public model\n",
    "BASE_MODEL = \"mosaicml/mpt-7b-instruct\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"Explain quantum mechanics in simple terms.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T18:59:03.904626Z",
     "iopub.status.busy": "2025-12-21T18:59:03.904301Z",
     "iopub.status.idle": "2025-12-21T19:01:01.865984Z",
     "shell.execute_reply": "2025-12-21T19:01:01.865255Z",
     "shell.execute_reply.started": "2025-12-21T18:59:03.904601Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# Full working MPT-7B-Instruct pipeline (no gated models)\n",
    "# ========================================================\n",
    "\n",
    "# Install dependencies (run once)\n",
    "# !pip install transformers accelerate datasets peft safetensors bitsandbytes --upgrade\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "import torch\n",
    "\n",
    "# -------------------------------\n",
    "# Configuration\n",
    "# -------------------------------\n",
    "BASE_MODEL = \"mosaicml/mpt-7b-instruct\"  # public model, no HF token needed\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_TOKENS = 200\n",
    "\n",
    "# -------------------------------\n",
    "# Load tokenizer\n",
    "# -------------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# -------------------------------\n",
    "# Load model\n",
    "# -------------------------------\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "\n",
    "# Ensure pad_token_id is set for generation\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# -------------------------------\n",
    "# Generation function\n",
    "# -------------------------------\n",
    "def generate_text(prompt: str, max_new_tokens: int = MAX_TOKENS):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    generation_config = GenerationConfig(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Example usage\n",
    "# -------------------------------\n",
    "prompt = \"Explain quantum mechanics in simple terms.\"\n",
    "result = generate_text(prompt)\n",
    "print(result)\n",
    "\n",
    "# -------------------------------\n",
    "# Optional: LoRA fine-tuning template\n",
    "# -------------------------------\n",
    "\"\"\"\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# Wrap the model with LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Training loop example\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_mpt7b\",\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T19:05:36.278726Z",
     "iopub.status.busy": "2025-12-21T19:05:36.278239Z",
     "iopub.status.idle": "2025-12-21T19:05:40.169543Z",
     "shell.execute_reply": "2025-12-21T19:05:40.168687Z",
     "shell.execute_reply.started": "2025-12-21T19:05:36.278697Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate datasets peft safetensors diffusers sentencepiece huggingface_hub pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T19:58:08.541612Z",
     "iopub.status.busy": "2025-12-21T19:58:08.540829Z",
     "iopub.status.idle": "2025-12-21T19:58:13.740276Z",
     "shell.execute_reply": "2025-12-21T19:58:13.739291Z",
     "shell.execute_reply.started": "2025-12-21T19:58:08.541562Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install transformers==4.52.0 accelerate safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T17:03:09.036166Z",
     "iopub.status.busy": "2025-12-22T17:03:09.035923Z",
     "iopub.status.idle": "2025-12-22T17:03:30.240069Z",
     "shell.execute_reply": "2025-12-22T17:03:30.239172Z",
     "shell.execute_reply.started": "2025-12-22T17:03:09.036132Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.52.0 datasets accelerate peft bitsandbytes huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T07:20:31.180370Z",
     "iopub.status.busy": "2025-12-24T07:20:31.179843Z",
     "iopub.status.idle": "2025-12-24T07:21:15.702150Z",
     "shell.execute_reply": "2025-12-24T07:21:15.701394Z",
     "shell.execute_reply.started": "2025-12-24T07:20:31.180341Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch, json\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoTokenizer,\n",
    "    LlavaForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(\"YOUR_HF_TOKEN_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T07:21:20.152991Z",
     "iopub.status.busy": "2025-12-24T07:21:20.151823Z",
     "iopub.status.idle": "2025-12-24T07:22:16.207638Z",
     "shell.execute_reply": "2025-12-24T07:22:16.207017Z",
     "shell.execute_reply.started": "2025-12-24T07:21:20.152952Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=False)\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T07:22:22.295093Z",
     "iopub.status.busy": "2025-12-24T07:22:22.294423Z",
     "iopub.status.idle": "2025-12-24T07:22:22.552936Z",
     "shell.execute_reply": "2025-12-24T07:22:22.552210Z",
     "shell.execute_reply.started": "2025-12-24T07:22:22.295064Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T07:22:25.507770Z",
     "iopub.status.busy": "2025-12-24T07:22:25.507336Z",
     "iopub.status.idle": "2025-12-24T07:22:28.090303Z",
     "shell.execute_reply": "2025-12-24T07:22:28.089686Z",
     "shell.execute_reply.started": "2025-12-24T07:22:25.507725Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/json-dataset/train (1).json\") as f:\n",
    "    raw = json.load(f)\n",
    "\n",
    "def build_text(example):\n",
    "    text = \"\"\n",
    "    for t in example[\"conversations\"]:\n",
    "        text += f\"{t['from'].upper()}: {t['value']}\\n\"\n",
    "    return {\n",
    "        \"image\": example[\"image\"],\n",
    "        \"text\": text\n",
    "    }\n",
    "\n",
    "dataset = Dataset.from_list([build_text(x) for x in raw])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T07:22:32.206691Z",
     "iopub.status.busy": "2025-12-24T07:22:32.206388Z",
     "iopub.status.idle": "2025-12-24T07:22:32.211136Z",
     "shell.execute_reply": "2025-12-24T07:22:32.210598Z",
     "shell.execute_reply.started": "2025-12-24T07:22:32.206663Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = [b[\"image\"] for b in batch]\n",
    "    texts  = [b[\"text\"] for b in batch]\n",
    "\n",
    "    processed = processor(\n",
    "        images=images,\n",
    "        text=texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    processed[\"labels\"] = processed[\"input_ids\"].clone()\n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T07:22:34.887699Z",
     "iopub.status.busy": "2025-12-24T07:22:34.886989Z",
     "iopub.status.idle": "2025-12-24T07:22:37.912365Z",
     "shell.execute_reply": "2025-12-24T07:22:37.911759Z",
     "shell.execute_reply.started": "2025-12-24T07:22:34.887668Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"llava-xray-lora\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=2,\n",
    "    fp16=True,\n",
    "\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"mohit311/LLaVA-data-json\",\n",
    "\n",
    "    remove_unused_columns=False,  # REQUIRED FOR MULTIMODAL\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T07:20:18.835616Z",
     "iopub.status.busy": "2025-12-24T07:20:18.835446Z",
     "iopub.status.idle": "2025-12-24T07:20:18.850422Z",
     "shell.execute_reply": "2025-12-24T07:20:18.849181Z",
     "shell.execute_reply.started": "2025-12-24T07:20:18.835599Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.push_to_hub()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T12:15:26.545646Z",
     "iopub.status.busy": "2025-12-26T12:15:26.545363Z",
     "iopub.status.idle": "2025-12-26T12:15:47.240309Z",
     "shell.execute_reply": "2025-12-26T12:15:47.239561Z",
     "shell.execute_reply.started": "2025-12-26T12:15:26.545622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y torch torchvision torchaudio transformers accelerate peft bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T12:21:20.687053Z",
     "iopub.status.busy": "2025-12-26T12:21:20.686477Z",
     "iopub.status.idle": "2025-12-26T12:21:39.402419Z",
     "shell.execute_reply": "2025-12-26T12:21:39.401607Z",
     "shell.execute_reply.started": "2025-12-26T12:21:20.687022Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U \\\n",
    "  transformers==4.40.2 \\\n",
    "  accelerate==0.27.2 \\\n",
    "  peft==0.10.0 \\\n",
    "  datasets \\\n",
    "  pillow \\\n",
    "  huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T12:23:36.738771Z",
     "iopub.status.busy": "2025-12-26T12:23:36.738015Z",
     "iopub.status.idle": "2025-12-26T12:23:36.904806Z",
     "shell.execute_reply": "2025-12-26T12:23:36.904070Z",
     "shell.execute_reply.started": "2025-12-26T12:23:36.738741Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"YOUR_HF_TOKEN_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T12:35:28.153803Z",
     "iopub.status.busy": "2025-12-26T12:35:28.153102Z",
     "iopub.status.idle": "2025-12-26T12:35:51.312357Z",
     "shell.execute_reply": "2025-12-26T12:35:51.311700Z",
     "shell.execute_reply.started": "2025-12-26T12:35:28.153759Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from PIL import Image\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    LlavaForConditionalGeneration,\n",
    "    LlavaProcessor,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(\"YOUR_HF_TOKEN_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T12:35:59.236841Z",
     "iopub.status.busy": "2025-12-26T12:35:59.235698Z",
     "iopub.status.idle": "2025-12-26T12:36:58.482873Z",
     "shell.execute_reply": "2025-12-26T12:36:58.482258Z",
     "shell.execute_reply.started": "2025-12-26T12:35:59.236804Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "MODEL_ID = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "processor = LlavaProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T12:37:51.708578Z",
     "iopub.status.busy": "2025-12-26T12:37:51.708214Z",
     "iopub.status.idle": "2025-12-26T12:37:52.011430Z",
     "shell.execute_reply": "2025-12-26T12:37:52.010604Z",
     "shell.execute_reply.started": "2025-12-26T12:37:51.708548Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T12:38:18.774182Z",
     "iopub.status.busy": "2025-12-26T12:38:18.773848Z",
     "iopub.status.idle": "2025-12-26T12:38:19.544443Z",
     "shell.execute_reply": "2025-12-26T12:38:19.543831Z",
     "shell.execute_reply.started": "2025-12-26T12:38:18.774153Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/json-dataset/train (1).json\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "dataset = Dataset.from_list(raw_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T12:38:33.083113Z",
     "iopub.status.busy": "2025-12-26T12:38:33.082325Z",
     "iopub.status.idle": "2025-12-26T12:38:33.088388Z",
     "shell.execute_reply": "2025-12-26T12:38:33.087780Z",
     "shell.execute_reply.started": "2025-12-26T12:38:33.083080Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LlavaDataCollator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        images = []\n",
    "        texts = []\n",
    "\n",
    "        for ex in batch:\n",
    "            img = Image.open(ex[\"image\"]).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "\n",
    "            conv = \"\"\n",
    "            for turn in ex[\"conversations\"]:\n",
    "                conv += f\"{turn['from'].upper()}: {turn['value']}\\n\"\n",
    "            texts.append(conv)\n",
    "\n",
    "        inputs = self.processor(\n",
    "            images=images,\n",
    "            text=texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=1024\n",
    "        )\n",
    "\n",
    "        inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T12:38:44.961371Z",
     "iopub.status.busy": "2025-12-26T12:38:44.961038Z",
     "iopub.status.idle": "2025-12-26T12:38:44.994752Z",
     "shell.execute_reply": "2025-12-26T12:38:44.994191Z",
     "shell.execute_reply.started": "2025-12-26T12:38:44.961342Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llava-med\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"mohit311/LLaVA-data-json\",\n",
    "    hub_strategy=\"checkpoint\",\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T12:39:01.126939Z",
     "iopub.status.busy": "2025-12-26T12:39:01.126638Z",
     "iopub.status.idle": "2025-12-26T12:39:01.213322Z",
     "shell.execute_reply": "2025-12-26T12:39:01.212722Z",
     "shell.execute_reply.started": "2025-12-26T12:39:01.126915Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=LlavaDataCollator(processor),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T12:39:13.159470Z",
     "iopub.status.busy": "2025-12-26T12:39:13.158637Z",
     "iopub.status.idle": "2025-12-26T21:59:04.747511Z",
     "shell.execute_reply": "2025-12-26T21:59:04.746663Z",
     "shell.execute_reply.started": "2025-12-26T12:39:13.159439Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.push_to_hub()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T09:22:23.752857Z",
     "iopub.status.busy": "2026-01-01T09:22:23.752107Z",
     "iopub.status.idle": "2026-01-01T09:22:25.767714Z",
     "shell.execute_reply": "2026-01-01T09:22:25.766438Z",
     "shell.execute_reply.started": "2026-01-01T09:22:23.752821Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "INPUT_JSON = \"/kaggle/input/json-dataset/train (1).json\"\n",
    "OUTPUT_JSON = \"/kaggle/working/train_clean.json\"\n",
    "\n",
    "def clean_text(t):\n",
    "    t = t.strip()\n",
    "    t = re.sub(r\"GPT:|USER:|ASSISTANT:\", \"\", t, flags=re.I)\n",
    "    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t)\n",
    "    return t.strip()\n",
    "\n",
    "clean_data = []\n",
    "\n",
    "with open(INPUT_JSON) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for ex in data:\n",
    "    image = ex[\"image\"]\n",
    "\n",
    "    findings = None\n",
    "    impression = None\n",
    "\n",
    "    for turn in ex[\"conversations\"]:\n",
    "        txt = turn[\"value\"]\n",
    "\n",
    "        if \"FINDINGS:\" in txt.upper():\n",
    "            findings = txt\n",
    "        if \"IMPRESSION:\" in txt.upper():\n",
    "            impression = txt\n",
    "\n",
    "    if findings and impression:\n",
    "        findings = clean_text(findings)\n",
    "        impression = clean_text(impression)\n",
    "\n",
    "        assistant = f\"\"\"FINDINGS:\n",
    "{findings.replace(\"FINDINGS:\", \"\").strip()}\n",
    "\n",
    "IMPRESSION:\n",
    "{impression.replace(\"IMPRESSION:\", \"\").strip()}\n",
    "\"\"\"\n",
    "\n",
    "        clean_data.append({\n",
    "            \"image\": image,\n",
    "            \"conversations\": [\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": \"<image>\\nGenerate a chest X-ray report.\"\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": assistant\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "\n",
    "print(f\"Clean samples: {len(clean_data)}\")\n",
    "\n",
    "with open(OUTPUT_JSON, \"w\") as f:\n",
    "    json.dump(clean_data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T09:22:29.747179Z",
     "iopub.status.busy": "2026-01-01T09:22:29.746825Z",
     "iopub.status.idle": "2026-01-01T09:22:29.754210Z",
     "shell.execute_reply": "2026-01-01T09:22:29.753084Z",
     "shell.execute_reply.started": "2026-01-01T09:22:29.747146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BASE_MODEL = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T16:22:45.751948Z",
     "iopub.status.busy": "2025-12-31T16:22:45.751629Z",
     "iopub.status.idle": "2025-12-31T16:22:52.139911Z",
     "shell.execute_reply": "2025-12-31T16:22:52.139261Z",
     "shell.execute_reply.started": "2025-12-31T16:22:45.751920Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    \"mohit311/LLaVA-data-json\",  # <-- NOT checkpoint-xxxx\n",
    "    is_trainable=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T09:19:32.306732Z",
     "iopub.status.busy": "2026-01-01T09:19:32.306388Z",
     "iopub.status.idle": "2026-01-01T09:19:34.502653Z",
     "shell.execute_reply": "2026-01-01T09:19:34.501713Z",
     "shell.execute_reply.started": "2026-01-01T09:19:32.306703Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "INPUT_JSON = \"/kaggle/input/json-dataset/train (1).json\"\n",
    "OUTPUT_JSON = \"/kaggle/working/train_clean.json\"\n",
    "\n",
    "def clean_text(t):\n",
    "    t = t.strip()\n",
    "    t = re.sub(r\"GPT:|USER:|ASSISTANT:\", \"\", t, flags=re.I)\n",
    "    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t)\n",
    "    return t.strip()\n",
    "\n",
    "clean_data = []\n",
    "\n",
    "with open(INPUT_JSON) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for ex in data:\n",
    "    image = ex[\"image\"]\n",
    "\n",
    "    findings = None\n",
    "    impression = None\n",
    "\n",
    "    for turn in ex[\"conversations\"]:\n",
    "        txt = turn[\"value\"]\n",
    "\n",
    "        if \"FINDINGS:\" in txt.upper():\n",
    "            findings = txt\n",
    "        if \"IMPRESSION:\" in txt.upper():\n",
    "            impression = txt\n",
    "\n",
    "    if findings and impression:\n",
    "        findings = clean_text(findings)\n",
    "        impression = clean_text(impression)\n",
    "\n",
    "        assistant = f\"\"\"FINDINGS:\n",
    "{findings.replace(\"FINDINGS:\", \"\").strip()}\n",
    "\n",
    "IMPRESSION:\n",
    "{impression.replace(\"IMPRESSION:\", \"\").strip()}\n",
    "\"\"\"\n",
    "\n",
    "        clean_data.append({\n",
    "            \"image\": image,\n",
    "            \"conversations\": [\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": \"<image>\\nGenerate a chest X-ray report.\"\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": assistant\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "\n",
    "print(f\"Clean samples: {len(clean_data)}\")\n",
    "\n",
    "with open(OUTPUT_JSON, \"w\") as f:\n",
    "    json.dump(clean_data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T09:23:33.807132Z",
     "iopub.status.busy": "2026-01-01T09:23:33.806270Z",
     "iopub.status.idle": "2026-01-01T09:23:36.731956Z",
     "shell.execute_reply": "2026-01-01T09:23:36.731180Z",
     "shell.execute_reply.started": "2026-01-01T09:23:33.807100Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "LOCAL_CKPT_DIR = \"./llava-med/checkpoint-5361\"\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"mohit311/LLaVA-data-json\",\n",
    "    local_dir=LOCAL_CKPT_DIR,\n",
    "    allow_patterns=[\n",
    "        \"last-checkpoint/*\",\n",
    "        \"adapter_*\",\n",
    "        \"optimizer.pt\",\n",
    "        \"scheduler.pt\",\n",
    "        \"trainer_state.json\",\n",
    "        \"training_args.bin\",\n",
    "        \"rng_state.pth\",\n",
    "    ],\n",
    "    local_dir_use_symlinks=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T09:26:12.507755Z",
     "iopub.status.busy": "2026-01-01T09:26:12.507425Z",
     "iopub.status.idle": "2026-01-01T09:26:30.039899Z",
     "shell.execute_reply": "2026-01-01T09:26:30.039173Z",
     "shell.execute_reply.started": "2026-01-01T09:26:12.507733Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers accelerate peft datasets pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T09:27:48.319846Z",
     "iopub.status.busy": "2026-01-01T09:27:48.319610Z",
     "iopub.status.idle": "2026-01-01T09:29:39.069518Z",
     "shell.execute_reply": "2026-01-01T09:29:39.068706Z",
     "shell.execute_reply.started": "2026-01-01T09:27:48.319826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch, json\n",
    "from PIL import Image\n",
    "from datasets import Dataset\n",
    "from transformers import LlavaForConditionalGeneration, LlavaProcessor, TrainingArguments, Trainer\n",
    "from peft import PeftModel\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(\"YOUR_HF_TOKEN_HERE",
    "\n",
    "BASE_MODEL = \"llava-hf/llava-1.5-7b-hf\"\n",
    "ADAPTER_REPO = \"mohit311/LLaVA-data-json\"\n",
    "\n",
    "# Processor\n",
    "processor = LlavaProcessor.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# Base model\n",
    "base_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load LoRA adapter from HF\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    ADAPTER_REPO,\n",
    "    is_trainable=True\n",
    ")\n",
    "\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T09:30:02.122469Z",
     "iopub.status.busy": "2026-01-01T09:30:02.121550Z",
     "iopub.status.idle": "2026-01-01T09:30:02.128263Z",
     "shell.execute_reply": "2026-01-01T09:30:02.127543Z",
     "shell.execute_reply.started": "2026-01-01T09:30:02.122429Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LlavaDataCollator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        images, texts = [], []\n",
    "\n",
    "        for ex in batch:\n",
    "            images.append(Image.open(ex[\"image\"]).convert(\"RGB\"))\n",
    "            user = ex[\"conversations\"][0][\"value\"]\n",
    "            assistant = ex[\"conversations\"][1][\"value\"]\n",
    "            texts.append(f\"USER: {user}\\nASSISTANT: {assistant}\")\n",
    "\n",
    "        inputs = self.processor(\n",
    "            images=images,\n",
    "            text=texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=768\n",
    "        )\n",
    "\n",
    "        inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T09:31:19.292238Z",
     "iopub.status.busy": "2026-01-01T09:31:19.291428Z",
     "iopub.status.idle": "2026-01-01T09:31:19.362075Z",
     "shell.execute_reply": "2026-01-01T09:31:19.361331Z",
     "shell.execute_reply.started": "2026-01-01T09:31:19.292205Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "LOCAL_CKPT_DIR = \"./llava-med/checkpoint-6500\"\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"mohit311/LLaVA-data-json\",\n",
    "    local_dir=LOCAL_CKPT_DIR,\n",
    "    allow_patterns=[\n",
    "        \"checkpoint-6500/*\",\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T09:31:35.303814Z",
     "iopub.status.busy": "2026-01-01T09:31:35.303500Z",
     "iopub.status.idle": "2026-01-01T09:31:44.291705Z",
     "shell.execute_reply": "2026-01-01T09:31:44.291063Z",
     "shell.execute_reply.started": "2026-01-01T09:31:35.303786Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlavaForConditionalGeneration, LlavaProcessor\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "BASE_MODEL = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "processor = LlavaProcessor.from_pretrained(BASE_MODEL)\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T09:31:55.998900Z",
     "iopub.status.busy": "2026-01-01T09:31:55.998284Z",
     "iopub.status.idle": "2026-01-01T09:31:56.646560Z",
     "shell.execute_reply": "2026-01-01T09:31:56.645865Z",
     "shell.execute_reply.started": "2026-01-01T09:31:55.998869Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "with open(\"/kaggle/input/train-clean/train_clean.json\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "dataset = Dataset.from_list(raw_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T09:32:04.956738Z",
     "iopub.status.busy": "2026-01-01T09:32:04.956346Z",
     "iopub.status.idle": "2026-01-01T09:32:04.962245Z",
     "shell.execute_reply": "2026-01-01T09:32:04.961614Z",
     "shell.execute_reply.started": "2026-01-01T09:32:04.956710Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class LlavaDataCollator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        images, texts = [], []\n",
    "\n",
    "        for ex in batch:\n",
    "            images.append(Image.open(ex[\"image\"]).convert(\"RGB\"))\n",
    "\n",
    "            user = ex[\"conversations\"][0][\"value\"]\n",
    "            assistant = ex[\"conversations\"][1][\"value\"]\n",
    "\n",
    "            texts.append(f\"USER: {user}\\nASSISTANT: {assistant}\")\n",
    "\n",
    "        inputs = self.processor(\n",
    "            images=images,\n",
    "            text=texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=1024\n",
    "        )\n",
    "\n",
    "        inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T09:32:13.554846Z",
     "iopub.status.busy": "2026-01-01T09:32:13.554343Z",
     "iopub.status.idle": "2026-01-01T09:32:13.588022Z",
     "shell.execute_reply": "2026-01-01T09:32:13.587478Z",
     "shell.execute_reply.started": "2026-01-01T09:32:13.554817Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llava-med\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=2,     # TOTAL epochs (not additional)\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"mohit311/LLaVA-data-json\",\n",
    "    hub_strategy=\"checkpoint\",\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T09:33:45.599924Z",
     "iopub.status.busy": "2026-01-01T09:33:45.599558Z",
     "iopub.status.idle": "2026-01-01T09:33:50.933389Z",
     "shell.execute_reply": "2026-01-01T09:33:50.932676Z",
     "shell.execute_reply.started": "2026-01-01T09:33:45.599896Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "LOCAL_CKPT_DIR = \"./llava-med/last-checkpoint\"\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"mohit311/LLaVA-data-json\",\n",
    "    local_dir=LOCAL_CKPT_DIR,\n",
    "    repo_type=\"model\",\n",
    "    allow_patterns=[\n",
    "        \"last-checkpoint/*\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Checkpoint downloaded to:\", LOCAL_CKPT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T09:38:08.233785Z",
     "iopub.status.busy": "2026-01-01T09:38:08.232990Z",
     "iopub.status.idle": "2026-01-01T09:38:09.333166Z",
     "shell.execute_reply": "2026-01-01T09:38:09.332530Z",
     "shell.execute_reply.started": "2026-01-01T09:38:08.233752Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "LOCAL_CKPT_DIR = \"./llava-med/checkpoint-6500\"\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"mohit311/LLaVA-data-json\",\n",
    "    local_dir=LOCAL_CKPT_DIR,\n",
    "    allow_patterns=[\n",
    "        \"adapter_model.safetensors\",\n",
    "        \"adapter_config.json\",\n",
    "        \"optimizer.pt\",\n",
    "        \"scheduler.pt\",\n",
    "        \"trainer_state.json\",\n",
    "        \"training_args.bin\",\n",
    "        \"rng_state.pth\"\n",
    "    ],\n",
    "    local_dir_use_symlinks=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T09:38:14.731209Z",
     "iopub.status.busy": "2026-01-01T09:38:14.730487Z",
     "iopub.status.idle": "2026-01-01T09:38:24.112006Z",
     "shell.execute_reply": "2026-01-01T09:38:24.111097Z",
     "shell.execute_reply.started": "2026-01-01T09:38:14.731178Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from PIL import Image\n",
    "from datasets import Dataset\n",
    "from transformers import LlavaForConditionalGeneration, LlavaProcessor, TrainingArguments, Trainer\n",
    "from peft import PeftModel\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(\"YOUR_HF_TOKEN_HERE",
    "\n",
    "BASE_MODEL = \"llava-hf/llava-1.5-7b-hf\"\n",
    "CKPT_DIR = \"./llava-med/checkpoint-6500\"\n",
    "\n",
    "processor = LlavaProcessor.from_pretrained(BASE_MODEL)\n",
    "\n",
    "base_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    CKPT_DIR,\n",
    "    is_trainable=True\n",
    ")\n",
    "\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T09:38:45.484642Z",
     "iopub.status.busy": "2026-01-01T09:38:45.484309Z",
     "iopub.status.idle": "2026-01-01T09:38:46.136404Z",
     "shell.execute_reply": "2026-01-01T09:38:46.135484Z",
     "shell.execute_reply.started": "2026-01-01T09:38:45.484615Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/train-clean/train_clean.json\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "dataset = Dataset.from_list(raw_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T09:52:37.756053Z",
     "iopub.status.busy": "2026-01-01T09:52:37.755746Z",
     "iopub.status.idle": "2026-01-01T17:11:04.986305Z",
     "shell.execute_reply": "2026-01-01T17:11:04.985630Z",
     "shell.execute_reply.started": "2026-01-01T09:52:37.756026Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#HLRS\n",
    "# =========================\n",
    "# RESUME & FINISH TRAINING\n",
    "# =========================\n",
    "\n",
    "import torch, json\n",
    "from PIL import Image\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    LlavaForConditionalGeneration,\n",
    "    LlavaProcessor,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import PeftModel\n",
    "from huggingface_hub import login\n",
    "\n",
    "# ---------------- LOGIN ----------------\n",
    "login(\"YOUR_HF_TOKEN_HERE",
    "\n",
    "BASE_MODEL = \"llava-hf/llava-1.5-7b-hf\"\n",
    "LORA_REPO = \"mohit311/LLaVA-data-json\"\n",
    "\n",
    "# ---------------- PROCESSOR ----------------\n",
    "processor = LlavaProcessor.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# ---------------- BASE MODEL ----------------\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# ---------------- LOAD TRAINED LoRA (step ~6500) ----------------\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    LORA_REPO,\n",
    "    is_trainable=True\n",
    ")\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# ---------------- DATASET ----------------\n",
    "with open(\"/kaggle/input/train-clean/train_clean.json\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "dataset = Dataset.from_list(raw_data)\n",
    "\n",
    "# ---------------- DATA COLLATOR ----------------\n",
    "class LlavaDataCollator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        images, texts = [], []\n",
    "\n",
    "        for ex in batch:\n",
    "            images.append(Image.open(ex[\"image\"]).convert(\"RGB\"))\n",
    "            text = f\"USER: {ex['conversations'][0]['value']}\\nASSISTANT: {ex['conversations'][1]['value']}\"\n",
    "            texts.append(text)\n",
    "\n",
    "        inputs = self.processor(\n",
    "            images=images,\n",
    "            text=texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=1024\n",
    "        )\n",
    "        inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n",
    "        return inputs\n",
    "\n",
    "# ---------------- TRAINING ARGS ----------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llava-med\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=0.7,   # FINISH REMAINING TRAINING\n",
    "    learning_rate=1e-4,     # lower LR for stability\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=LORA_REPO,\n",
    "    hub_strategy=\"end\",\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# ---------------- TRAIN ----------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=LlavaDataCollator(processor),\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.push_to_hub()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T18:19:16.622211Z",
     "iopub.status.busy": "2026-01-01T18:19:16.621885Z",
     "iopub.status.idle": "2026-01-01T18:21:10.526304Z",
     "shell.execute_reply": "2026-01-01T18:21:10.525627Z",
     "shell.execute_reply.started": "2026-01-01T18:19:16.622188Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import LlavaForConditionalGeneration, LlavaProcessor\n",
    "from peft import PeftModel\n",
    "from huggingface_hub import login\n",
    "\n",
    "# ---------------- LOGIN ----------------\n",
    "login(\"YOUR_HF_TOKEN_HERE",
    "\n",
    "# ---------------- MODEL IDS ----------------\n",
    "BASE_MODEL = \"llava-hf/llava-1.5-7b-hf\"\n",
    "LORA_REPO  = \"mohit311/LLaVA-data-json\"\n",
    "\n",
    "# ---------------- DEVICE ----------------\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---------------- LOAD PROCESSOR ----------------\n",
    "processor = LlavaProcessor.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# ---------------- LOAD BASE MODEL (SINGLE GPU) ----------------\n",
    "base_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": device}\n",
    ").eval()\n",
    "\n",
    "# ---------------- LOAD LoRA ADAPTER ----------------\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    LORA_REPO,\n",
    "    is_trainable=False\n",
    ").eval()\n",
    "\n",
    "# ---------------- LOAD IMAGE ----------------\n",
    "image_path = \"/kaggle/input/mimic-cxr/Main2/p1000/i1000.jpg\"   # CHANGE THIS\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# ---------------- PROMPT (MUST CONTAIN <image>) ----------------\n",
    "prompt = \"\"\"<image>\n",
    "You are a radiologist.\n",
    "\n",
    "Write a chest X-ray report with exactly two sections.\n",
    "\n",
    "FINDINGS:\n",
    "Describe only what is visible on the image.\n",
    "\n",
    "IMPRESSION:\n",
    "Provide a concise clinical summary.\n",
    "\n",
    "Do not repeat sections.\n",
    "Do not add history.\n",
    "\"\"\"\n",
    "\n",
    "# ---------------- PROCESS INPUT ----------------\n",
    "inputs = processor(\n",
    "    images=image,\n",
    "    text=prompt,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "# ---------------- GENERATE ----------------\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "# ---------------- DECODE ----------------\n",
    "output = processor.tokenizer.decode(\n",
    "    output_ids[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(\"===== MODEL OUTPUT =====\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T18:22:26.910626Z",
     "iopub.status.busy": "2026-01-01T18:22:26.909837Z",
     "iopub.status.idle": "2026-01-01T18:22:26.915528Z",
     "shell.execute_reply": "2026-01-01T18:22:26.914740Z",
     "shell.execute_reply.started": "2026-01-01T18:22:26.910594Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_report(text):\n",
    "    if \"FINDINGS:\" in text:\n",
    "        text = text[text.index(\"FINDINGS:\"):]\n",
    "    text = text.split(\"FINDINGS:\", 1)[0] + \"FINDINGS:\" + text.split(\"FINDINGS:\", 1)[1]\n",
    "    return text.strip()\n",
    "\n",
    "print(clean_report(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T04:38:37.891536Z",
     "iopub.status.busy": "2026-01-05T04:38:37.891225Z",
     "iopub.status.idle": "2026-01-05T04:40:11.732963Z",
     "shell.execute_reply": "2026-01-05T04:40:11.732299Z",
     "shell.execute_reply.started": "2026-01-05T04:38:37.891513Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import LlavaForConditionalGeneration, LlavaProcessor\n",
    "from peft import PeftModel\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "BASE_MODEL = \"llava-hf/llava-1.5-7b-hf\"\n",
    "LORA_REPO = \"mohit311/LLaVA-data-json\"\n",
    "IMAGE_PATH = \"/kaggle/input/mimic-cxr/Main2/p10003/i10003.jpg\"  # change this\n",
    "\n",
    "DEVICE = \"cuda:0\"\n",
    "\n",
    "# ---------------- LOAD PROCESSOR ----------------\n",
    "processor = LlavaProcessor.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# ---------------- LOAD BASE MODEL (SINGLE GPU) ----------------\n",
    "base_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0}   # force single GPU\n",
    ").eval()\n",
    "\n",
    "# ---------------- LOAD LORA ADAPTER ----------------\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    LORA_REPO,\n",
    "    is_trainable=False\n",
    ").eval()\n",
    "\n",
    "# ---------------- LOAD IMAGE ----------------\n",
    "image = Image.open(IMAGE_PATH).convert(\"RGB\")\n",
    "\n",
    "# ---------------- PROMPT (CRITICAL) ----------------\n",
    "prompt = \"\"\"<image>\n",
    "You are an expert radiologist.\n",
    "\n",
    "Generate a chest X-ray report using EXACTLY this format:\n",
    "\n",
    "FINDINGS:\n",
    "- Describe lungs, pleura, heart size, mediastinum, lines or devices if visible.\n",
    "\n",
    "IMPRESSION:\n",
    "- Provide a concise clinical summary (1-2 sentences).\n",
    "\n",
    "Return exactly ONE report.\n",
    "\"\"\"\n",
    "\n",
    "# ---------------- PREPARE INPUTS ----------------\n",
    "inputs = processor(\n",
    "    text=prompt,\n",
    "    images=image,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "# ---------------- GENERATE ----------------\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=220,\n",
    "        do_sample=False,      # important\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        num_beams=1,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "\n",
    "output = processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"===== MODEL OUTPUT =====\\n\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T06:54:00.702012Z",
     "iopub.status.busy": "2026-01-05T06:54:00.701329Z",
     "iopub.status.idle": "2026-01-05T06:54:09.725756Z",
     "shell.execute_reply": "2026-01-05T06:54:09.725041Z",
     "shell.execute_reply.started": "2026-01-05T06:54:00.701975Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install sqlalchemy --upgrade\n",
    "!pip install gradio transformers peft torch pillow accelerate protobuf -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T06:54:36.963804Z",
     "iopub.status.busy": "2026-01-05T06:54:36.963501Z",
     "iopub.status.idle": "2026-01-05T06:54:58.583642Z",
     "shell.execute_reply": "2026-01-05T06:54:58.582798Z",
     "shell.execute_reply.started": "2026-01-05T06:54:36.963772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import LlavaForConditionalGeneration, LlavaProcessor\n",
    "from peft import PeftModel\n",
    "import gc\n",
    "\n",
    "# Global variables to cache model\n",
    "model = None\n",
    "processor = None\n",
    "device = None\n",
    "\n",
    "def load_model_once():\n",
    "    \"\"\"Load model once on startup\"\"\"\n",
    "    global model, processor, device\n",
    "    \n",
    "    if model is not None:\n",
    "        return\n",
    "    \n",
    "    print(\"Loading processor...\")\n",
    "    processor = LlavaProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "    \n",
    "    print(\"Loading base model...\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "    \n",
    "    base_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "        \"llava-hf/llava-1.5-7b-hf\",\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    print(\"Loading LoRA adapter...\")\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        \"mohit311/LLaVA-data-json\",\n",
    "        is_trainable=False\n",
    "    ).eval()\n",
    "    \n",
    "    print(f\"Model loaded on {device.upper()}!\")\n",
    "\n",
    "def diagnose_image(image, question):\n",
    "    \"\"\"Generate diagnosis from image and question\"\"\"\n",
    "    \n",
    "    if image is None:\n",
    "        return \"Please upload an image first!\"\n",
    "    \n",
    "    if not question or question.strip() == \"\":\n",
    "        return \"Please enter a question!\"\n",
    "    \n",
    "    try:\n",
    "        # Ensure image is RGB\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = f\"<image>\\n{question}\"\n",
    "        \n",
    "        # Process inputs\n",
    "        inputs = processor(\n",
    "            text=prompt,\n",
    "            images=image,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        inputs = {k: v.to(device) if hasattr(v, 'to') else v for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=220,\n",
    "                do_sample=False,\n",
    "                temperature=0.0,\n",
    "                top_p=1.0,\n",
    "                num_beams=1,\n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "        \n",
    "        # Decode output\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        new_tokens = output_ids[0, input_length:]\n",
    "        response = processor.decode(new_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        # Clean up\n",
    "        del inputs, output_ids\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        gc.collect()\n",
    "        \n",
    "        return response.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Load model on startup\n",
    "load_model_once()\n",
    "\n",
    "# Create Gradio interface\n",
    "with gr.Blocks(title=\"Medical Image Diagnosis\") as demo:\n",
    "    gr.Markdown(\"# Medical Image Diagnosis Assistant\")\n",
    "    gr.Markdown(\"Upload a medical image and ask questions about it using AI analysis powered by LLaVA 1.5 7B with medical LoRA fine-tuning.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### Upload Image\")\n",
    "            image_input = gr.Image(label=\"Medical Image\", type=\"pil\")\n",
    "            \n",
    "            gr.Markdown(\"### Ask Your Question\")\n",
    "            question_input = gr.Textbox(\n",
    "                label=\"Your Question\",\n",
    "                placeholder=\"e.g., What abnormalities do you see in this chest X-ray?\",\n",
    "                lines=3\n",
    "            )\n",
    "            \n",
    "            submit_button = gr.Button(\"Analyze Image\", variant=\"primary\")\n",
    "        \n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### AI Response\")\n",
    "            output_text = gr.Textbox(\n",
    "                label=\"Diagnosis\",\n",
    "                lines=10,\n",
    "                interactive=False\n",
    "            )\n",
    "    \n",
    "    # Example questions\n",
    "    gr.Markdown(\"### Example Questions\")\n",
    "    example_questions = [\n",
    "        \"What is shown in this medical image?\",\n",
    "        \"What abnormalities can you identify?\",\n",
    "        \"Describe the key findings in this image.\",\n",
    "        \"What anatomical structures are visible?\",\n",
    "        \"Are there any signs of disease or pathology?\"\n",
    "    ]\n",
    "    \n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            [None, q] for q in example_questions\n",
    "        ],\n",
    "        inputs=[image_input, question_input],\n",
    "        label=\"Try these questions (add your own image)\"\n",
    "    )\n",
    "    \n",
    "    # Connect button\n",
    "    submit_button.click(\n",
    "        fn=diagnose_image,\n",
    "        inputs=[image_input, question_input],\n",
    "        outputs=output_text\n",
    "    )\n",
    "    \n",
    "    gr.Markdown(\"---\")\n",
    "    gr.Markdown(\"\"\"\n",
    "    ### Technical Details\n",
    "    - **Model**: LLaVA 1.5 7B\n",
    "    - **Fine-tuning**: LoRA adapter (medical imaging)\n",
    "    - **Device**: GPU (T4 on Kaggle / Colab)\n",
    "    \n",
    "    **Disclaimer**: For educational purposes only. Not a substitute for professional medical advice.\n",
    "    \"\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)  # share=True generates a public link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T06:55:13.340493Z",
     "iopub.status.busy": "2026-01-05T06:55:13.339730Z",
     "iopub.status.idle": "2026-01-05T06:55:13.787038Z",
     "shell.execute_reply": "2026-01-05T06:55:13.786434Z",
     "shell.execute_reply.started": "2026-01-05T06:55:13.340459Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T17:40:25.237832Z",
     "iopub.status.busy": "2026-01-05T17:40:25.237514Z",
     "iopub.status.idle": "2026-01-05T17:40:33.547766Z",
     "shell.execute_reply": "2026-01-05T17:40:33.546719Z",
     "shell.execute_reply.started": "2026-01-05T17:40:25.237803Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q evaluate rouge-score nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T10:14:13.612293Z",
     "iopub.status.busy": "2026-01-05T10:14:13.611475Z",
     "iopub.status.idle": "2026-01-05T11:14:34.836953Z",
     "shell.execute_reply": "2026-01-05T11:14:34.835861Z",
     "shell.execute_reply.started": "2026-01-05T10:14:13.612258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "from PIL import Image\n",
    "from transformers import LlavaForConditionalGeneration, LlavaProcessor\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "BASE_MODEL = \"llava-hf/llava-1.5-7b-hf\"\n",
    "LORA_REPO = \"mohit311/LLaVA-data-json\"\n",
    "DATA_PATH = \"/kaggle/input/train-clean/train_clean.json\"\n",
    "MAX_SAMPLES = 200   # start small (increase later)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---------------- LOAD METRICS ----------------\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# ---------------- LOAD MODEL ----------------\n",
    "processor = LlavaProcessor.from_pretrained(BASE_MODEL)\n",
    "\n",
    "base_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    LORA_REPO\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# ---------------- LOAD DATA ----------------\n",
    "dataset = load_dataset(\"json\", data_files=DATA_PATH)[\"train\"]\n",
    "dataset = dataset.select(range(MAX_SAMPLES))\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "# ---------------- INFERENCE LOOP ----------------\n",
    "for sample in dataset:\n",
    "    image = Image.open(sample[\"image\"]).convert(\"RGB\")\n",
    "\n",
    "    prompt = sample[\"conversations\"][0][\"value\"]\n",
    "\n",
    "    inputs = processor(\n",
    "        images=image,\n",
    "        text=prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    pred = processor.decode(\n",
    "        output_ids[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    gt = sample[\"conversations\"][1][\"value\"]\n",
    "\n",
    "    predictions.append(pred)\n",
    "    references.append([gt])  # BLEU expects list of references\n",
    "\n",
    "# ---------------- COMPUTE METRICS ----------------\n",
    "bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "rouge_score = rouge.compute(predictions=predictions, references=[r[0] for r in references])\n",
    "\n",
    "print(\"===== EVALUATION RESULTS =====\")\n",
    "print(\"BLEU:\", bleu_score)\n",
    "print(\"ROUGE:\", rouge_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T16:48:26.907028Z",
     "iopub.status.busy": "2026-01-05T16:48:26.906704Z",
     "iopub.status.idle": "2026-01-05T16:55:04.338818Z",
     "shell.execute_reply": "2026-01-05T16:55:04.338212Z",
     "shell.execute_reply.started": "2026-01-05T16:48:26.906996Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "from PIL import Image\n",
    "from transformers import LlavaForConditionalGeneration, LlavaProcessor\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ================= CONFIG =================\n",
    "BASE_MODEL = \"llava-hf/llava-1.5-7b-hf\"\n",
    "LORA_REPO = \"mohit311/LLaVA-data-json\"\n",
    "DATA_PATH = \"/kaggle/input/train-clean/train_clean.json\"\n",
    "\n",
    "MAX_SAMPLES = 50\n",
    "MAX_NEW_TOKENS = 64\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# ================= METRICS =================\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# ================= PROCESSOR =================\n",
    "processor = LlavaProcessor.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# ================= DATA =================\n",
    "dataset = load_dataset(\"json\", data_files=DATA_PATH)[\"train\"]\n",
    "dataset = dataset.select(range(MAX_SAMPLES))\n",
    "\n",
    "# ================= EVAL FUNCTION =================\n",
    "def run_eval(model, name):\n",
    "    model.eval()\n",
    "    preds, refs = [], []\n",
    "\n",
    "    for sample in tqdm(dataset, desc=name):\n",
    "        image = Image.open(sample[\"image\"]).convert(\"RGB\")\n",
    "        prompt = sample[\"conversations\"][0][\"value\"]\n",
    "        gt = sample[\"conversations\"][1][\"value\"]\n",
    "\n",
    "        inputs = processor(\n",
    "            images=image,\n",
    "            text=prompt,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                do_sample=False,\n",
    "                use_cache=True\n",
    "            )\n",
    "\n",
    "        pred = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "        preds.append(pred)\n",
    "        refs.append([gt])\n",
    "\n",
    "    return (\n",
    "        bleu.compute(predictions=preds, references=refs),\n",
    "        rouge.compute(predictions=preds, references=[r[0] for r in refs])\n",
    "    )\n",
    "\n",
    "# ================= BASELINE =================\n",
    "baseline = LlavaForConditionalGeneration.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "\n",
    "baseline_bleu, baseline_rouge = run_eval(baseline, \"Baseline\")\n",
    "\n",
    "del baseline\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ================= FINETUNED =================\n",
    "base = LlavaForConditionalGeneration.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "\n",
    "finetuned = PeftModel.from_pretrained(base, LORA_REPO)\n",
    "finetuned_bleu, finetuned_rouge = run_eval(finetuned, \"Finetuned\")\n",
    "\n",
    "# ================= RESULTS =================\n",
    "print(\"\\n===== BASELINE =====\")\n",
    "print(\"BLEU:\", baseline_bleu)\n",
    "print(\"ROUGE:\", baseline_rouge)\n",
    "\n",
    "print(\"\\n===== FINETUNED =====\")\n",
    "print(\"BLEU:\", finetuned_bleu)\n",
    "print(\"ROUGE:\", finetuned_rouge)\n",
    "\n",
    "print(\"\\n===== DELTA =====\")\n",
    "print(\" BLEU:\", finetuned_bleu[\"bleu\"] - baseline_bleu[\"bleu\"])\n",
    "print(\" ROUGE-L:\", finetuned_rouge[\"rougeL\"] - baseline_rouge[\"rougeL\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T17:44:27.660302Z",
     "iopub.status.busy": "2026-01-05T17:44:27.660075Z",
     "iopub.status.idle": "2026-01-05T18:09:16.417547Z",
     "shell.execute_reply": "2026-01-05T18:09:16.416755Z",
     "shell.execute_reply.started": "2026-01-05T17:44:27.660272Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Pathology-level F1 Evaluation\n",
    "# Baseline vs Finetuned LLaVA\n",
    "# ==============================\n",
    "\n",
    "import torch\n",
    "import re\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import LlavaForConditionalGeneration, LlavaProcessor\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "BASE_MODEL = \"llava-hf/llava-1.5-7b-hf\"\n",
    "LORA_REPO = \"mohit311/LLaVA-data-json\"\n",
    "DATA_PATH = \"/kaggle/input/train-clean/train_clean.json\"\n",
    "\n",
    "MAX_SAMPLES = 50\n",
    "MAX_NEW_TOKENS = 256\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---------------- PATHOLOGY LIST ----------------\n",
    "PATHOLOGIES = [\n",
    "    \"atelectasis\", \"consolidation\", \"pneumonia\", \"edema\",\n",
    "    \"pleural effusion\", \"cardiomegaly\", \"pneumothorax\",\n",
    "    \"lung opacity\", \"support devices\"\n",
    "]\n",
    "\n",
    "# ---------------- LOAD PROCESSOR ----------------\n",
    "processor = LlavaProcessor.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# ---------------- LOAD DATA ----------------\n",
    "dataset = load_dataset(\"json\", data_files=DATA_PATH)[\"train\"]\n",
    "dataset = dataset.select(range(MAX_SAMPLES))\n",
    "\n",
    "# ---------------- PATHOLOGY EXTRACTION ----------------\n",
    "def extract_pathologies(text):\n",
    "    text = text.lower()\n",
    "    found = set()\n",
    "    for p in PATHOLOGIES:\n",
    "        if re.search(rf\"\\b{re.escape(p)}\\b\", text):\n",
    "            found.add(p)\n",
    "    return found\n",
    "\n",
    "# ---------------- REPORT GENERATION ----------------\n",
    "def generate_reports(model, name):\n",
    "    preds, refs = [], []\n",
    "\n",
    "    for sample in tqdm(dataset, desc=name):\n",
    "        image = Image.open(sample[\"image\"]).convert(\"RGB\")\n",
    "        prompt = sample[\"conversations\"][0][\"value\"]\n",
    "        gt = sample[\"conversations\"][1][\"value\"]\n",
    "\n",
    "        inputs = processor(\n",
    "            images=image,\n",
    "            text=prompt,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                do_sample=False\n",
    "            )\n",
    "\n",
    "        pred = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "        preds.append(pred)\n",
    "        refs.append(gt)\n",
    "\n",
    "    return preds, refs\n",
    "\n",
    "# ---------------- PATHOLOGY F1 ----------------\n",
    "def pathology_f1(preds, refs):\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for p, r in zip(preds, refs):\n",
    "        p_set = extract_pathologies(p)\n",
    "        r_set = extract_pathologies(r)\n",
    "\n",
    "        for path in PATHOLOGIES:\n",
    "            y_true.append(int(path in r_set))\n",
    "            y_pred.append(int(path in p_set))\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"binary\", zero_division=0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1\": f1\n",
    "    }\n",
    "\n",
    "# ================= BASELINE MODEL =================\n",
    "baseline_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16\n",
    ").to(DEVICE)\n",
    "\n",
    "baseline_model.eval()\n",
    "\n",
    "baseline_preds, baseline_refs = generate_reports(baseline_model, \"Baseline\")\n",
    "baseline_metrics = pathology_f1(baseline_preds, baseline_refs)\n",
    "\n",
    "# ---- FREE GPU AFTER BASELINE ----\n",
    "del baseline_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ================= FINETUNED MODEL =================\n",
    "finetuned_base = LlavaForConditionalGeneration.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16\n",
    ").to(DEVICE)\n",
    "\n",
    "finetuned_model = PeftModel.from_pretrained(\n",
    "    finetuned_base,\n",
    "    LORA_REPO\n",
    ")\n",
    "\n",
    "finetuned_model.eval()\n",
    "\n",
    "finetuned_preds, finetuned_refs = generate_reports(finetuned_model, \"Finetuned\")\n",
    "finetuned_metrics = pathology_f1(finetuned_preds, finetuned_refs)\n",
    "\n",
    "# ================= RESULTS =================\n",
    "print(\"\\n===== PATHOLOGY-LEVEL RESULTS =====\\n\")\n",
    "\n",
    "print(\"Baseline:\")\n",
    "for k, v in baseline_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nFinetuned:\")\n",
    "for k, v in finetuned_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nDelta (Finetuned - Baseline):\")\n",
    "print(f\" Precision: {finetuned_metrics['Precision'] - baseline_metrics['Precision']:.4f}\")\n",
    "print(f\" Recall:    {finetuned_metrics['Recall'] - baseline_metrics['Recall']:.4f}\")\n",
    "print(f\" F1:        {finetuned_metrics['F1'] - baseline_metrics['F1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T18:14:39.826584Z",
     "iopub.status.busy": "2026-01-05T18:14:39.825811Z",
     "iopub.status.idle": "2026-01-05T18:14:39.963493Z",
     "shell.execute_reply": "2026-01-05T18:14:39.962775Z",
     "shell.execute_reply.started": "2026-01-05T18:14:39.826552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ============================\n",
    "# INSERT YOUR FINAL SCORES\n",
    "# ============================\n",
    "\n",
    "results = {\n",
    "    \"Baseline\": {\n",
    "        \"BLEU\": 0.000008079,\n",
    "        \"ROUGE-L\": 0.04207,\n",
    "        \"Pathology Precision\": 0.0000,\n",
    "        \"Pathology Recall\": 0.0000,\n",
    "        \"Pathology F1\": 0.0000,\n",
    "    },\n",
    "    \"Finetuned (Ours)\": {\n",
    "        \"BLEU\": 0.007122,\n",
    "        \"ROUGE-L\": 0.15310,\n",
    "        \"Pathology Precision\": 0.4425,\n",
    "        \"Pathology Recall\": 0.3817,\n",
    "        \"Pathology F1\": 0.4098,\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# BUILD COMPARISON TABLE\n",
    "# ============================\n",
    "\n",
    "df = pd.DataFrame(results).T\n",
    "df[\" (Ours - Baseline)\"] = df.loc[\"Finetuned (Ours)\"] - df.loc[\"Baseline\"]\n",
    "\n",
    "# Reorder columns (paper-style)\n",
    "df = df[\n",
    "    [\"BLEU\", \"ROUGE-L\", \"Pathology Precision\", \"Pathology Recall\", \"Pathology F1\", \" (Ours - Baseline)\"]\n",
    "]\n",
    "\n",
    "# Round for publication\n",
    "df = df.round(4)\n",
    "\n",
    "print(\"\\n===== PAPER-STYLE COMPARISON TABLE =====\\n\")\n",
    "display(df)\n",
    "\n",
    "# ============================\n",
    "# OPTIONAL: EXPORT\n",
    "# ============================\n",
    "\n",
    "# CSV (for Excel / plotting)\n",
    "df.to_csv(\"comparison_table.csv\")\n",
    "\n",
    "# LaTeX (for paper)\n",
    "latex_table = df.to_latex(\n",
    "    caption=\"Comparison of Baseline LLaVA and Fine-tuned Model on Chest X-ray Report Generation\",\n",
    "    label=\"tab:comparison\",\n",
    "    bold_rows=True\n",
    ")\n",
    "\n",
    "with open(\"comparison_table.tex\", \"w\") as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" - comparison_table.csv\")\n",
    "print(\" - comparison_table.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T18:19:08.362121Z",
     "iopub.status.busy": "2026-01-05T18:19:08.361466Z",
     "iopub.status.idle": "2026-01-05T18:20:42.930768Z",
     "shell.execute_reply": "2026-01-05T18:20:42.930144Z",
     "shell.execute_reply.started": "2026-01-05T18:19:08.362089Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================\n",
    "# CONFIG\n",
    "# =============================\n",
    "PATHOLOGIES = [\n",
    "    \"cardiomegaly\",\n",
    "    \"consolidation\",\n",
    "    \"edema\",\n",
    "    \"pleural effusion\",\n",
    "    \"pneumonia\",\n",
    "    \"pneumothorax\",\n",
    "]\n",
    "\n",
    "N_BOOTSTRAP = 1000\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# =============================\n",
    "# PATHOLOGY EXTRACTION\n",
    "# =============================\n",
    "def extract_labels(text):\n",
    "    text = text.lower()\n",
    "    labels = {}\n",
    "    for p in PATHOLOGIES:\n",
    "        labels[p] = int(bool(re.search(rf\"\\b{p}\\b\", text)))\n",
    "    return labels\n",
    "\n",
    "# =============================\n",
    "# METRIC COMPUTATION\n",
    "# =============================\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn + 1e-8)\n",
    "    sens = tp / (tp + fn + 1e-8)\n",
    "    spec = tn / (tn + fp + 1e-8)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    return acc, sens, spec, f1\n",
    "\n",
    "def bootstrap_ci(y_true, y_pred, metric_fn):\n",
    "    scores = []\n",
    "    n = len(y_true)\n",
    "    for _ in range(N_BOOTSTRAP):\n",
    "        idx = np.random.choice(n, n, replace=True)\n",
    "        scores.append(metric_fn(y_true[idx], y_pred[idx]))\n",
    "    return np.percentile(scores, [2.5, 97.5])\n",
    "\n",
    "# =============================\n",
    "# MAIN EVALUATION FUNCTION\n",
    "# =============================\n",
    "def evaluate_model(predictions, references, model_name):\n",
    "    rows = []\n",
    "\n",
    "    for pathology in PATHOLOGIES:\n",
    "        y_true = np.array([extract_labels(r)[pathology] for r in references])\n",
    "        y_pred = np.array([extract_labels(p)[pathology] for p in predictions])\n",
    "\n",
    "        acc, sens, spec, f1 = compute_metrics(y_true, y_pred)\n",
    "\n",
    "        acc_ci = bootstrap_ci(y_true, y_pred, lambda a,b: compute_metrics(a,b)[0])\n",
    "        sens_ci = bootstrap_ci(y_true, y_pred, lambda a,b: compute_metrics(a,b)[1])\n",
    "        spec_ci = bootstrap_ci(y_true, y_pred, lambda a,b: compute_metrics(a,b)[2])\n",
    "        f1_ci = bootstrap_ci(y_true, y_pred, lambda a,b: compute_metrics(a,b)[3])\n",
    "\n",
    "        rows.append({\n",
    "            \"Metric\": \"Accuracy\",\n",
    "            \"Pathology\": pathology.title(),\n",
    "            model_name: f\"{acc:.2f} ({acc_ci[0]:.2f}, {acc_ci[1]:.2f})\"\n",
    "        })\n",
    "        rows.append({\n",
    "            \"Metric\": \"Sensitivity\",\n",
    "            \"Pathology\": pathology.title(),\n",
    "            model_name: f\"{sens:.2f} ({sens_ci[0]:.2f}, {sens_ci[1]:.2f})\"\n",
    "        })\n",
    "        rows.append({\n",
    "            \"Metric\": \"Specificity\",\n",
    "            \"Pathology\": pathology.title(),\n",
    "            model_name: f\"{spec:.2f} ({spec_ci[0]:.2f}, {spec_ci[1]:.2f})\"\n",
    "        })\n",
    "        rows.append({\n",
    "            \"Metric\": \"F1 score\",\n",
    "            \"Pathology\": pathology.title(),\n",
    "            model_name: f\"{f1:.2f} ({f1_ci[0]:.2f}, {f1_ci[1]:.2f})\"\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# ENSURE REFERENCES EXIST\n",
    "# =============================\n",
    "assert len(baseline_preds) == len(finetuned_preds), \"Prediction length mismatch\"\n",
    "\n",
    "references = []\n",
    "for sample in dataset:\n",
    "    references.append(sample[\"conversations\"][1][\"value\"])\n",
    "\n",
    "assert len(references) == len(baseline_preds), \"References do not match predictions\"\n",
    "# =============================\n",
    "# RUN EVALUATION\n",
    "# =============================\n",
    "baseline_df = evaluate_model(baseline_preds, references, \"Baseline\")\n",
    "finetuned_df = evaluate_model(finetuned_preds, references, \"Finetuned (Ours)\")\n",
    "\n",
    "# Merge like Table 4\n",
    "table4 = baseline_df.merge(\n",
    "    finetuned_df,\n",
    "    on=[\"Metric\", \"Pathology\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"\\n===== TABLE 4 STYLE RESULTS =====\\n\")\n",
    "display(table4)\n",
    "\n",
    "# =============================\n",
    "# EXPORT\n",
    "# =============================\n",
    "table4.to_csv(\"table4_pathology_results.csv\", index=False)\n",
    "\n",
    "latex = table4.to_latex(\n",
    "    index=False,\n",
    "    caption=\"Model performance by pathology on external test set\",\n",
    "    label=\"tab:pathology_results\"\n",
    ")\n",
    "\n",
    "with open(\"table4_pathology_results.tex\", \"w\") as f:\n",
    "    f.write(latex)\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" - table4_pathology_results.csv\")\n",
    "print(\" - table4_pathology_results.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T15:58:31.770727Z",
     "iopub.status.busy": "2026-01-09T15:58:31.770540Z",
     "iopub.status.idle": "2026-01-09T15:58:43.053438Z",
     "shell.execute_reply": "2026-01-09T15:58:43.052449Z",
     "shell.execute_reply.started": "2026-01-09T15:58:31.770707Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TABLE-4 STYLE PATHOLOGY EVALUATION (SINGLE GPU, STABLE)\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import LlavaForConditionalGeneration, LlavaProcessor\n",
    "from peft import PeftModel\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "BASE_MODEL = \"llava-hf/llava-1.5-7b-hf\"\n",
    "LORA_REPO = \"mohit311/LLaVA-data-json\"\n",
    "VAL_PATH = \"/kaggle/input/json-dataset/val (1).json\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\")   # <<< FORCE SINGLE GPU\n",
    "SUBSET_SIZE = 300\n",
    "MAX_NEW_TOKENS = 200\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "PATHOLOGIES = [\n",
    "    \"cardiomegaly\",\n",
    "    \"consolidation\",\n",
    "    \"edema\",\n",
    "    \"pleural effusion\",\n",
    "    \"pneumonia\",\n",
    "    \"pneumothorax\",\n",
    "]\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ---------------- LOAD PROCESSOR ----------------\n",
    "processor = LlavaProcessor.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# ---------------- LOAD MODEL (NO AUTO DEVICE MAP) ----------------\n",
    "base_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=None,     # <<< CRITICAL\n",
    ")\n",
    "\n",
    "base_model.to(DEVICE)\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    LORA_REPO\n",
    ")\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# ---------------- LOAD & SUBSET DATA ----------------\n",
    "dataset = load_dataset(\"json\", data_files=VAL_PATH)[\"train\"]\n",
    "\n",
    "indices = np.random.choice(len(dataset), SUBSET_SIZE, replace=False)\n",
    "dataset = dataset.select(indices.tolist())\n",
    "\n",
    "# ---------------- PATHOLOGY EXTRACTION ----------------\n",
    "def extract_labels(text):\n",
    "    text = text.lower()\n",
    "    return {p: int(p in text) for p in PATHOLOGIES}\n",
    "\n",
    "# ---------------- METRICS ----------------\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "\n",
    "    acc = (tp + tn) / max(tp + tn + fp + fn, 1)\n",
    "    sens = tp / max(tp + fn, 1)\n",
    "    spec = tn / max(tn + fp, 1)\n",
    "    f1 = (2 * tp) / max((2 * tp + fp + fn), 1)\n",
    "\n",
    "    acc_ci = proportion_confint(tp + tn, tp + tn + fp + fn, method=\"wilson\")\n",
    "    sens_ci = proportion_confint(tp, tp + fn, method=\"wilson\") if (tp + fn) > 0 else (0,0)\n",
    "    spec_ci = proportion_confint(tn, tn + fp, method=\"wilson\") if (tn + fp) > 0 else (0,0)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": (acc, acc_ci),\n",
    "        \"Sensitivity\": (sens, sens_ci),\n",
    "        \"Specificity\": (spec, spec_ci),\n",
    "        \"F1 score\": (f1, (0,0)),\n",
    "    }\n",
    "\n",
    "# ---------------- RUN EVALUATION ----------------\n",
    "y_true = {p: [] for p in PATHOLOGIES}\n",
    "y_pred = {p: [] for p in PATHOLOGIES}\n",
    "\n",
    "print(f\"\\nEvaluating OUR model on {SUBSET_SIZE} validation samples (single GPU)...\\n\")\n",
    "\n",
    "for sample in tqdm(dataset, desc=\"Evaluating Ours\"):\n",
    "    image = Image.open(sample[\"image\"]).convert(\"RGB\")\n",
    "    prompt = sample[\"conversations\"][0][\"value\"]\n",
    "    gt_text = sample[\"conversations\"][1][\"value\"]\n",
    "\n",
    "    inputs = processor(\n",
    "        images=image,\n",
    "        text=prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    pred_text = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    gt_labels = extract_labels(gt_text)\n",
    "    pred_labels = extract_labels(pred_text)\n",
    "\n",
    "    for p in PATHOLOGIES:\n",
    "        y_true[p].append(gt_labels[p])\n",
    "        y_pred[p].append(pred_labels[p])\n",
    "\n",
    "# ---------------- BUILD TABLE 4 ----------------\n",
    "rows = []\n",
    "\n",
    "for pathology in PATHOLOGIES:\n",
    "    metrics = compute_metrics(y_true[pathology], y_pred[pathology])\n",
    "\n",
    "    for metric, (val, ci) in metrics.items():\n",
    "        rows.append({\n",
    "            \"Metric\": metric,\n",
    "            \"Pathology\": pathology.title(),\n",
    "            \"CXR-LLaVA (Ours)\": f\"{val:.2f} ({ci[0]:.2f}, {ci[1]:.2f})\",\n",
    "            \"GPT-4V (Paper)\": \"\",\n",
    "            \"Gemini-Pro (Paper)\": \"\"\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# ---------------- OUTPUT ----------------\n",
    "print(\"\\n===== TABLE 4 STYLE RESULTS (VALIDATION SUBSET) =====\\n\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "df.to_csv(\"table4_validation_subset_ours.csv\", index=False)\n",
    "df.to_latex(\"table4_validation_subset_ours.tex\", index=False)\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" - table4_validation_subset_ours.csv\")\n",
    "print(\" - table4_validation_subset_ours.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T16:02:27.811448Z",
     "iopub.status.busy": "2026-01-09T16:02:27.811138Z",
     "iopub.status.idle": "2026-01-09T20:24:11.738066Z",
     "shell.execute_reply": "2026-01-09T20:24:11.737276Z",
     "shell.execute_reply.started": "2026-01-09T16:02:27.811419Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TABLE-4 STYLE PATHOLOGY EVALUATION (SINGLE GPU, STABLE)\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import LlavaForConditionalGeneration, LlavaProcessor\n",
    "from peft import PeftModel\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "BASE_MODEL = \"llava-hf/llava-1.5-7b-hf\"\n",
    "LORA_REPO = \"mohit311/LLaVA-data-json\"\n",
    "VAL_PATH = \"/kaggle/input/json-dataset/val (1).json\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\")   # <<< FORCE SINGLE GPU\n",
    "SUBSET_SIZE = 500\n",
    "MAX_NEW_TOKENS = 400\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "PATHOLOGIES = [\n",
    "    \"cardiomegaly\",\n",
    "    \"consolidation\",\n",
    "    \"edema\",\n",
    "    \"pleural effusion\",\n",
    "    \"pneumonia\",\n",
    "    \"pneumothorax\",\n",
    "]\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ---------------- LOAD PROCESSOR ----------------\n",
    "processor = LlavaProcessor.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# ---------------- LOAD MODEL (NO AUTO DEVICE MAP) ----------------\n",
    "base_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=None,     # <<< CRITICAL\n",
    ")\n",
    "\n",
    "base_model.to(DEVICE)\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    LORA_REPO\n",
    ")\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# ---------------- LOAD & SUBSET DATA ----------------\n",
    "dataset = load_dataset(\"json\", data_files=VAL_PATH)[\"train\"]\n",
    "\n",
    "indices = np.random.choice(len(dataset), SUBSET_SIZE, replace=False)\n",
    "dataset = dataset.select(indices.tolist())\n",
    "\n",
    "# ---------------- PATHOLOGY EXTRACTION ----------------\n",
    "def extract_labels(text):\n",
    "    text = text.lower()\n",
    "    return {p: int(p in text) for p in PATHOLOGIES}\n",
    "\n",
    "# ---------------- METRICS ----------------\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "\n",
    "    acc = (tp + tn) / max(tp + tn + fp + fn, 1)\n",
    "    sens = tp / max(tp + fn, 1)\n",
    "    spec = tn / max(tn + fp, 1)\n",
    "    f1 = (2 * tp) / max((2 * tp + fp + fn), 1)\n",
    "\n",
    "    acc_ci = proportion_confint(tp + tn, tp + tn + fp + fn, method=\"wilson\")\n",
    "    sens_ci = proportion_confint(tp, tp + fn, method=\"wilson\") if (tp + fn) > 0 else (0,0)\n",
    "    spec_ci = proportion_confint(tn, tn + fp, method=\"wilson\") if (tn + fp) > 0 else (0,0)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": (acc, acc_ci),\n",
    "        \"Sensitivity\": (sens, sens_ci),\n",
    "        \"Specificity\": (spec, spec_ci),\n",
    "        \"F1 score\": (f1, (0,0)),\n",
    "    }\n",
    "\n",
    "# ---------------- RUN EVALUATION ----------------\n",
    "y_true = {p: [] for p in PATHOLOGIES}\n",
    "y_pred = {p: [] for p in PATHOLOGIES}\n",
    "\n",
    "print(f\"\\nEvaluating OUR model on {SUBSET_SIZE} validation samples (single GPU)...\\n\")\n",
    "\n",
    "for sample in tqdm(dataset, desc=\"Evaluating Ours\"):\n",
    "    image = Image.open(sample[\"image\"]).convert(\"RGB\")\n",
    "    prompt = sample[\"conversations\"][0][\"value\"]\n",
    "    gt_text = sample[\"conversations\"][1][\"value\"]\n",
    "\n",
    "    inputs = processor(\n",
    "        images=image,\n",
    "        text=prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    pred_text = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    gt_labels = extract_labels(gt_text)\n",
    "    pred_labels = extract_labels(pred_text)\n",
    "\n",
    "    for p in PATHOLOGIES:\n",
    "        y_true[p].append(gt_labels[p])\n",
    "        y_pred[p].append(pred_labels[p])\n",
    "\n",
    "# ---------------- BUILD TABLE 4 ----------------\n",
    "rows = []\n",
    "\n",
    "for pathology in PATHOLOGIES:\n",
    "    metrics = compute_metrics(y_true[pathology], y_pred[pathology])\n",
    "\n",
    "    for metric, (val, ci) in metrics.items():\n",
    "        rows.append({\n",
    "            \"Metric\": metric,\n",
    "            \"Pathology\": pathology.title(),\n",
    "            \"CXR-LLaVA (Ours)\": f\"{val:.2f} ({ci[0]:.2f}, {ci[1]:.2f})\",\n",
    "            \"GPT-4V (Paper)\": \"\",\n",
    "            \"Gemini-Pro (Paper)\": \"\"\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# ---------------- OUTPUT ----------------\n",
    "print(\"\\n===== TABLE 4 STYLE RESULTS (VALIDATION SUBSET) =====\\n\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "df.to_csv(\"table4_validation_subset_ours.csv\", index=False)\n",
    "df.to_latex(\"table4_validation_subset_ours.tex\", index=False)\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" - table4_validation_subset_ours.csv\")\n",
    "print(\" - table4_validation_subset_ours.tex\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8940565,
     "sourceId": 14043691,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8940786,
     "sourceId": 14198587,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9083452,
     "sourceId": 14237750,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9170338,
     "sourceId": 14361239,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}